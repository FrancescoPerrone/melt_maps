{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt5\n",
    "\n",
    "# handling XRF HDF5 data\n",
    "import h5py \n",
    "import hyperspy.api as hs\n",
    "\n",
    "#handling data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# data vis\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#import glob\n",
    "#import scipy\n",
    "\n",
    "# data processing, dimension reduction and clustering\n",
    "from sklearn.decomposition import PCA, FactorAnalysis\n",
    "import sklearn as skl\n",
    "from sklearn.pipeline import Pipeline\n",
    "from skcmeans.algorithms import Probabilistic, GustafsonKesselMixin\n",
    "\n",
    "#from skcmeans.algorithms import Probabilistic, GustafsonKesselMixin\n",
    "#from skcmeans.algorithms import Probabilistic\n",
    "#from skcmeans.algorithms import Probabilistic, GustafsonKesselMixin\n",
    "# from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert XRF data to Hyperspy standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mISE_500sqaures_A21-016_Map1_001.h5\u001b[m\u001b[m\n",
      "\u001b[31mISE_500sqaures_A21_054_botom_right_map_center_001.h5\u001b[m\u001b[m\n",
      "map.hspy\n",
      "map1.hspy\n",
      "map<class 'map'>.hspy\n",
      "/Users/user/Documents/github/melt_maps\n"
     ]
    }
   ],
   "source": [
    "!cd '/Users/user/Documents/Projects/active/working/XRF_machine_learning/data'\n",
    "!ls '/Users/user/Documents/Projects/active/working/XRF_machine_learning/data'\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = '/Users/user/Documents/Projects/active/working/XRF_machine_learning/data/ISE_500sqaures_A21_054_botom_right_map_center_001.h5'\n",
    "save_path = '/Users/user/Documents/Projects/active/working/XRF_machine_learning/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_and_list_h5_objects(obj, level=0, path='', dataset_list=[]):\n",
    "    \"\"\"\n",
    "   checks whether the input object is a file or group, prints its name/type \n",
    "   accordingly, and then goes through its subgroups and datasets, printing \n",
    "   their names/types and adding the dataset paths to the list 'dataset_list'.\n",
    "   \n",
    "   Returns\n",
    "    -------\n",
    "    prints the structure of the HDF5 file and returns a list of dataset paths\n",
    "    \n",
    "   Note\n",
    "    -----\n",
    "    This function makes it easier to look through the HDF5 blackbox and call \n",
    "    datasets.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, h5py.File):\n",
    "        print(\"File name: {}\".format(obj.filename))\n",
    "        obj = obj['/']\n",
    "    elif isinstance(obj, h5py.Group):\n",
    "        print(\"Group: {}\".format(obj.name))\n",
    "\n",
    "    for key, val in obj.items():\n",
    "        new_path = path + '/' + key if path else key\n",
    "        if isinstance(val, h5py.Group):\n",
    "            print(\"  \" * level + \"Group: {}\".format(key))\n",
    "            print_and_list_h5_objects(val, level + 1, new_path, dataset_list)\n",
    "        else:\n",
    "            print(\"  \" * level + \"Dataset: {}\".format(key))\n",
    "            dataset_list.append(new_path)\n",
    "    \n",
    "    return dataset_list\n",
    "\n",
    "# USE EXAMPLE\n",
    "# with h5py.File(data,'r') as h5:\n",
    "#    datasets = print_and_list_h5_objects(h5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File name: /Users/user/Documents/Projects/active/working/XRF_machine_learning/data/ISE_500sqaures_A21_054_botom_right_map_center_001.h5\n",
      "Group: xrmmap\n",
      "Group: /xrmmap\n",
      "  Group: areas\n",
      "Group: /xrmmap/areas\n",
      "    Dataset: A21-054_Br_Xanes_spot\n",
      "    Dataset: A21-054_Br_Xanes_spot_2\n",
      "    Dataset: A21-054_I_Xanes_spot\n",
      "    Dataset: A21-054_I_Xanes_spot_2\n",
      "    Dataset: area_003\n",
      "    Dataset: area_004\n",
      "  Group: config\n",
      "Group: /xrmmap/config\n",
      "    Group: environ\n",
      "Group: /xrmmap/config/environ\n",
      "      Dataset: address\n",
      "      Dataset: name\n",
      "      Dataset: value\n",
      "    Group: general\n",
      "Group: /xrmmap/config/general\n",
      "      Dataset: basedir\n",
      "      Dataset: envfile\n",
      "    Group: mca_calib\n",
      "Group: /xrmmap/config/mca_calib\n",
      "      Dataset: offset\n",
      "      Dataset: quad\n",
      "      Dataset: slope\n",
      "    Group: mca_settings\n",
      "Group: /xrmmap/config/mca_settings\n",
      "    Group: motor_controller\n",
      "Group: /xrmmap/config/motor_controller\n",
      "      Dataset: group\n",
      "      Dataset: host\n",
      "      Dataset: mode\n",
      "      Dataset: passwd\n",
      "      Dataset: positioners\n",
      "      Dataset: type\n",
      "      Dataset: user\n",
      "    Group: notes\n",
      "Group: /xrmmap/config/notes\n",
      "    Group: positioners\n",
      "Group: /xrmmap/config/positioners\n",
      "      Dataset: 13IDE:En:Energy\n",
      "      Dataset: 13IDE:m19\n",
      "      Dataset: 13IDE:m25\n",
      "      Dataset: 13IDE:m28\n",
      "      Dataset: 13IDE:m31\n",
      "      Dataset: 13IDE:m32\n",
      "      Dataset: 13IDE:m34\n",
      "      Dataset: 13IDE:m35\n",
      "      Dataset: 13IDE:m36\n",
      "      Dataset: 13IDE:m39\n",
      "      Dataset: 13IDE:m4\n",
      "      Dataset: 13IDE:m40\n",
      "      Dataset: 13IDE:userTran5.A\n",
      "      Dataset: 13XRM:ANA:Energy\n",
      "      Dataset: 13XRM:m1\n",
      "      Dataset: 13XRM:m10\n",
      "      Dataset: 13XRM:m11\n",
      "      Dataset: 13XRM:m2\n",
      "      Dataset: 13XRM:m3\n",
      "      Dataset: 13XRM:m4\n",
      "      Dataset: 13XRM:m5\n",
      "      Dataset: 13XRM:m6\n",
      "    Group: rois\n",
      "Group: /xrmmap/config/rois\n",
      "      Dataset: address\n",
      "      Dataset: limits\n",
      "      Dataset: name\n",
      "    Group: scan\n",
      "Group: /xrmmap/config/scan\n",
      "      Dataset: comments\n",
      "      Dataset: dimension\n",
      "      Dataset: filename\n",
      "      Dataset: pos1\n",
      "      Dataset: pos2\n",
      "      Dataset: start1\n",
      "      Dataset: start2\n",
      "      Dataset: step1\n",
      "      Dataset: step2\n",
      "      Dataset: stop1\n",
      "      Dataset: stop2\n",
      "      Dataset: text\n",
      "      Dataset: time1\n",
      "  Group: mcasum\n",
      "Group: /xrmmap/mcasum\n",
      "    Dataset: counts\n",
      "    Dataset: dtfactor\n",
      "    Dataset: energy\n",
      "    Dataset: inpcounts\n",
      "    Dataset: livetime\n",
      "    Dataset: outcounts\n",
      "    Dataset: realtime\n",
      "  Group: positions\n",
      "Group: /xrmmap/positions\n",
      "    Dataset: address\n",
      "    Dataset: name\n",
      "    Dataset: pos\n",
      "  Group: roimap\n",
      "Group: /xrmmap/roimap\n",
      "    Dataset: det_address\n",
      "    Dataset: det_cor\n",
      "    Dataset: det_name\n",
      "    Dataset: det_raw\n",
      "    Group: mcasum\n",
      "Group: /xrmmap/roimap/mcasum\n",
      "      Group: As Ka\n",
      "Group: /xrmmap/roimap/mcasum/As Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Ba La\n",
      "Group: /xrmmap/roimap/mcasum/Ba La\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Br Ka\n",
      "Group: /xrmmap/roimap/mcasum/Br Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Ca Ka\n",
      "Group: /xrmmap/roimap/mcasum/Ca Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Ce La\n",
      "Group: /xrmmap/roimap/mcasum/Ce La\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Cl Ka\n",
      "Group: /xrmmap/roimap/mcasum/Cl Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Co Ka\n",
      "Group: /xrmmap/roimap/mcasum/Co Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Cr Ka\n",
      "Group: /xrmmap/roimap/mcasum/Cr Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Cu Ka\n",
      "Group: /xrmmap/roimap/mcasum/Cu Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Eu La\n",
      "Group: /xrmmap/roimap/mcasum/Eu La\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Fe Ka\n",
      "Group: /xrmmap/roimap/mcasum/Fe Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Ge Ka\n",
      "Group: /xrmmap/roimap/mcasum/Ge Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: I La1\n",
      "Group: /xrmmap/roimap/mcasum/I La1\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: I Lb1\n",
      "Group: /xrmmap/roimap/mcasum/I Lb1\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: K Ka\n",
      "Group: /xrmmap/roimap/mcasum/K Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Mn Ka\n",
      "Group: /xrmmap/roimap/mcasum/Mn Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Mo Ka\n",
      "Group: /xrmmap/roimap/mcasum/Mo Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Ni Ka\n",
      "Group: /xrmmap/roimap/mcasum/Ni Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: OutputCounts\n",
      "Group: /xrmmap/roimap/mcasum/OutputCounts\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: P Ka\n",
      "Group: /xrmmap/roimap/mcasum/P Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Pb La1\n",
      "Group: /xrmmap/roimap/mcasum/Pb La1\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Rb Ka\n",
      "Group: /xrmmap/roimap/mcasum/Rb Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Re La1\n",
      "Group: /xrmmap/roimap/mcasum/Re La1\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: S Ka\n",
      "Group: /xrmmap/roimap/mcasum/S Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Sc Ka\n",
      "Group: /xrmmap/roimap/mcasum/Sc Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Se Ka\n",
      "Group: /xrmmap/roimap/mcasum/Se Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Si Ka\n",
      "Group: /xrmmap/roimap/mcasum/Si Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Sr Ka\n",
      "Group: /xrmmap/roimap/mcasum/Sr Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Ti Ka\n",
      "Group: /xrmmap/roimap/mcasum/Ti Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: V Ka\n",
      "Group: /xrmmap/roimap/mcasum/V Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: V Kb\n",
      "Group: /xrmmap/roimap/mcasum/V Kb\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Y Ka\n",
      "Group: /xrmmap/roimap/mcasum/Y Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Zn Ka\n",
      "Group: /xrmmap/roimap/mcasum/Zn Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Zr Ka\n",
      "Group: /xrmmap/roimap/mcasum/Zr Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Zr L\n",
      "Group: /xrmmap/roimap/mcasum/Zr L\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "    Dataset: sum_cor\n",
      "    Dataset: sum_list\n",
      "    Dataset: sum_name\n",
      "    Dataset: sum_raw\n",
      "  Group: scalars\n",
      "Group: /xrmmap/scalars\n",
      "    Dataset: I0\n",
      "    Dataset: I0_raw\n",
      "    Dataset: I1\n",
      "    Dataset: I1_raw\n",
      "    Dataset: I2\n",
      "    Dataset: I2_raw\n",
      "    Dataset: TSCALER\n",
      "    Dataset: TSCALER_raw\n",
      "  Group: work\n",
      "Group: /xrmmap/work\n",
      "  Group: xrd1d\n",
      "Group: /xrmmap/xrd1d\n"
     ]
    }
   ],
   "source": [
    "# prints the h5 file structure\n",
    "with h5py.File(data,'r') as h5:\n",
    "    datasets = print_and_list_h5_objects(h5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracts congiuration settings of the run\n",
    "needed to correctly scale axes and the scale channels to KeV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     0         0\n",
      "0                 Experiment.User_Name      Ezad\n",
      "1           Experiment.Proposal_Number     78722\n",
      "2        Experiment.Beam_Size__Nominal       2um\n",
      "3     Experiment.Monochromator_Crystal   Si(111)\n",
      "4   Experiment.Double_H_Mirror_Stripes   rhodium\n",
      "..                                 ...       ...\n",
      "60                 SampleStage.CoarseY  177.9370\n",
      "61                   SampleStage.FineX  -0.25500\n",
      "62                   SampleStage.FineY         0\n",
      "63                   SampleStage.Theta   90.0000\n",
      "64                    Undulator.Energy    13.717\n",
      "\n",
      "[65 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# extracts the congiuration settings of the scan and beamline\n",
    "with h5py.File(data, 'r') as h5:\n",
    "    dataset_df = pd.DataFrame(h5['/xrmmap/config/environ/name'][:])\n",
    "    value_df = pd.DataFrame(h5['/xrmmap/config/environ/value'][:])\n",
    "    # Merges configuration name and value dataframes\n",
    "    values_df = pd.concat([dataset_df, value_df], axis=1)\n",
    "    # Decodes byte strings to regular strings\n",
    "    values_df = values_df.apply(lambda x: x.str.decode('utf-8') if isinstance(x[0], bytes) else x, axis=0)\n",
    "print(values_df)\n",
    "# Save dataframe to CSV file\n",
    "#values_df.to_csv('values_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Axis management and scaling\n",
    "\n",
    "these are needed to scale the data and pixelscorrectly etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(201, 201, 4096)\n"
     ]
    }
   ],
   "source": [
    "# extracts xrf map and channels from the HDF5 file\n",
    "with h5py.File(data, 'r') as h5:\n",
    "    xrf_data = h5['/xrmmap/mcasum/counts'][:]\n",
    "    shape = h5['/xrmmap/mcasum/counts'].shape\n",
    "print(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts xrf map to hyperspy and scales data\n",
    "\n",
    "def convert_xrf_to_hyperspy(xrf_data):\n",
    "    # converts xrf map to hyperspy\n",
    "    xrf_map = hs.signals.Signal1D(xrf_data)\n",
    "\n",
    "    # scales the x, y and theta of the hyperspy data\n",
    "    xrf_map.axes_manager.navigation_axes[0].name = 'x'\n",
    "    xrf_map.axes_manager.navigation_axes[0].units = 'mm'\n",
    "    xrf_map.axes_manager.navigation_axes[0].scale = 0.025\n",
    "\n",
    "    xrf_map.axes_manager.navigation_axes[1].name = 'y'\n",
    "    xrf_map.axes_manager.navigation_axes[1].units = 'mm'\n",
    "    xrf_map.axes_manager.navigation_axes[1].scale = 0.025\n",
    "\n",
    "    xrf_map.axes_manager.signal_axes[0].name = 'Energy'\n",
    "    xrf_map.axes_manager.signal_axes[0].units = 'KeV'\n",
    "    xrf_map.axes_manager.signal_axes[0].scale = 13.717/4096\n",
    "    \n",
    "    return xrf_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Axes manager, axes: (201, 201|4096)>\n",
      "            Name |   size |  index |  offset |   scale |  units \n",
      "================ | ====== | ====== | ======= | ======= | ====== \n",
      "               x |    201 |      0 |       0 |   0.025 |     mm \n",
      "               y |    201 |      0 |       0 |   0.025 |     mm \n",
      "---------------- | ------ | ------ | ------- | ------- | ------ \n",
      "          Energy |   4096 |      0 |       0 |  0.0033 |    KeV \n"
     ]
    }
   ],
   "source": [
    "xrf_map = convert_xrf_to_hyperspy(xrf_data)\n",
    "print(xrf_map.axes_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwrite '/Users/user/Documents/Projects/active/working/XRF_machine_learning/data/map.hspy' (y/n)?\n",
      "y\n"
     ]
    }
   ],
   "source": [
    "xrf_map.save(save_path+'map.hspy'.format(map))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension reduction\n",
    "can start from here once the data has been pulled from the hdf5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 115.66 ms\n"
     ]
    }
   ],
   "source": [
    "xrf_stack = hs.load(save_path + 'map.hspy', stack = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Axes manager, axes: (201, 201|4096)>\n",
      "            Name |   size |  index |  offset |   scale |  units \n",
      "================ | ====== | ====== | ======= | ======= | ====== \n",
      "               x |    201 |      0 |       0 |   0.025 |     mm \n",
      "               y |    201 |      0 |       0 |   0.025 |     mm \n",
      "---------------- | ------ | ------ | ------- | ------- | ------ \n",
      "          Energy |   4096 |      0 |       0 |  0.0033 |    KeV \n"
     ]
    }
   ],
   "source": [
    "print(xrf_stack.axes_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIC function\n",
    "from Francesco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bic_elbow(signal, decomposition_method='PCA', n_components_range=np.arange(2, 11)):\n",
    "    \"\"\"\n",
    "    Perform a BIC elbow test to find the optimal number of components for a signal decomposition method.\n",
    "    Parameters\n",
    "    ----------\n",
    "    signal : `hyperspy.signals.Signal2D` or `hyperspy.signals.SignalND`\n",
    "        The hyperspy signal to perform the BIC elbow test on.\n",
    "    decomposition_method : str, optional\n",
    "        The signal decomposition method to use. Default is 'PCA'.\n",
    "    n_components_range : array-like, optional\n",
    "        The range of possible numbers of components to fit the signal model for.\n",
    "        Default is np.arange(2, 11).\n",
    "    Returns\n",
    "    -------\n",
    "    optimal_n_components : int\n",
    "        The optimal number of components that minimizes the BIC score.\n",
    "    Notes\n",
    "    -----\n",
    "    This function uses the Bayesian Information Criterion (BIC) to select the optimal number of components\n",
    "    for the given signal decomposition method. The BIC score is computed for each number of components\n",
    "    in the range, and the elbow point in the BIC curve is identified as the optimal number of components.\n",
    "    \"\"\"\n",
    "    bic_scores = []\n",
    "    for n_components in n_components_range:\n",
    "        # Fit the signal model\n",
    "        if decomposition_method == 'PCA':\n",
    "            model = signal.decomposition.PCA(n_components=n_components)\n",
    "        elif decomposition_method == 'NMF':\n",
    "            model = signal.decomposition.nmf(n_components=n_components)\n",
    "        elif decomposition_method == 'ICA':\n",
    "            model = signal.decomposition.ica(n_components=n_components)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown decomposition method: {decomposition_method}\")\n",
    "\n",
    "        # Compute the log-likelihood and number of model parameters\n",
    "        log_likelihood = model.score(signal)\n",
    "        n_samples, n_features = signal.data.shape\n",
    "        n_params = n_components * (n_features + 1)\n",
    "\n",
    "        # Compute the BIC score\n",
    "        bic = -2 * log_likelihood + n_params * np.log(n_samples)\n",
    "        bic_scores.append(bic)\n",
    "\n",
    "    # Plot the BIC scores as a function of the number of components\n",
    "    plt.plot(n_components_range, bic_scores, 'o-')\n",
    "    plt.xlabel('Number of components')\n",
    "    plt.ylabel('BIC score')\n",
    "    plt.title(f'BIC elbow test for {decomposition_method} decomposition')\n",
    "    plt.show()\n",
    "\n",
    "    # Find the elbow point\n",
    "    diff = np.diff(bic_scores)\n",
    "    elbow_index = np.argmax(diff) + 1\n",
    "    optimal_n_components = n_components_range[elbow_index]\n",
    "    print(f'The optimal number of components is {optimal_n_components}')\n",
    "\n",
    "    return optimal_n_components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposition info:\n",
      "  normalize_poissonian_noise=True\n",
      "  algorithm=Pipeline(steps=[('pca', PCA(n_components=40, svd_solver='full'))])\n",
      "  output_dimension=40\n",
      "  centre=None\n",
      "scikit-learn estimator:\n",
      "Pipeline(steps=[('pca', PCA(n_components=40, svd_solver='full'))])\n"
     ]
    }
   ],
   "source": [
    "PCA = Pipeline([(\"pca\", PCA(n_components=40, svd_solver='full'))])\n",
    "xrf_decomp_pca = xrf_stack.decomposition(normalize_poissonian_noise = True, algorithm = PCA, \n",
    "                        return_info = True, output_dimension = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: title={'center': 'data\\nPCA Scree Plot'}, xlabel='Principal component index', ylabel='Proportion of variance'>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xrf_stack.plot_cumulative_explained_variance_ratio()\n",
    "xrf_stack.plot_explained_variance_ratio(log=True, vline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_components = xrf_stack.estimate_elbow_position() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposition info:\n",
      "  normalize_poissonian_noise=True\n",
      "  algorithm=Pipeline(steps=[('pca', PCA(n_components=40, svd_solver='full'))])\n",
      "  output_dimension=7\n",
      "  centre=None\n",
      "scikit-learn estimator:\n",
      "Pipeline(steps=[('pca', PCA(n_components=40, svd_solver='full'))])\n"
     ]
    }
   ],
   "source": [
    "xrf_decomp_pca = xrf_stack.decomposition(normalize_poissonian_noise = True, algorithm = PCA, \n",
    "                        return_info = True, output_dimension = sig_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BaseSignal' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m pca_loadings \u001b[38;5;241m=\u001b[39m xrf_stack\u001b[38;5;241m.\u001b[39mget_decomposition_loadings()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mpca_loadings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BaseSignal' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "pca_loadings = xrf_stack.get_decomposition_loadings()\n",
    "pca_loadings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pca_loading_array' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m pca_loadings_array \u001b[38;5;241m=\u001b[39m pca_loadings\u001b[38;5;241m.\u001b[39mdata\n\u001b[0;32m----> 2\u001b[0m \u001b[43mpca_loading_array\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pca_loading_array' is not defined"
     ]
    }
   ],
   "source": [
    "pca_loadings_array = pca_loadings.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 7, 201)\n"
     ]
    }
   ],
   "source": [
    "sig_loadings = pca_loadings_array[:, :sig_components]\n",
    "np.save('sig_loadings.npy', sig_loadings)\n",
    "print(sig_loadings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of rows and columns for subplot grid\n",
    "num_pcs = sig_components - 1\n",
    "num_cols = 3\n",
    "if num_pcs > 0:\n",
    "    num_rows = int(np.ceil(num_pcs / num_cols))\n",
    "else:\n",
    "    num_rows = 1\n",
    "    num_cols = 1\n",
    "\n",
    "# figure; grid of subplots\n",
    "fig, axs = plt.subplots(num_rows, num_cols, figsize=(10, 3*num_rows))\n",
    "\n",
    "# plots the data on subplots\n",
    "for i in range(num_pcs):\n",
    "    row = i // num_cols\n",
    "    col = i % num_cols\n",
    "    axs[row, col].scatter(sig_loadings[:, 0], sig_loadings[:, i+1], s=5)\n",
    "    axs[row, col].set_xlabel(\"PC 1\")\n",
    "    axs[row, col].set_ylabel(\"PC {}\".format(i+2))\n",
    "\n",
    "# removes subplots without data\n",
    "for i in range(num_pcs, num_rows*num_cols):\n",
    "    axs.flat[i].remove()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# saves figure\n",
    "plt.savefig('pca.png', dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_loads, ydim, xdim = pca_load.data.shape\n",
    "\n",
    "fact_load_vect= pd.DataFrame((fa_load.data.reshape(fa_facts, ydim*xdim).T), columns = ['Factor 1','Factor 2','Factor 3','Factor 4', 'Factor 5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49, 201)\n"
     ]
    }
   ],
   "source": [
    "# Reshape into 2D\n",
    "n_samples = sig_loadings.shape[0] * sig_loadings.shape[1]\n",
    "n_features = sig_loadings.shape[2]\n",
    "sig_loadings_2d = sig_loadings.reshape(n_samples, n_features)\n",
    "print(sig_loadings_2d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposition info:\n",
      "  normalize_poissonian_noise=True\n",
      "  algorithm=Pipeline(steps=[('fa', FactorAnalysis(n_components=5))])\n",
      "  output_dimension=5\n",
      "  centre=None\n",
      "scikit-learn estimator:\n",
      "Pipeline(steps=[('fa', FactorAnalysis(n_components=5))])\n"
     ]
    }
   ],
   "source": [
    "factors = 5 # we need a way to determine factors\n",
    "pipeline = Pipeline([(\"fa\", FactorAnalysis(n_components = factors))])\n",
    "xrf_decomp = xrf_stack.decomposition(normalize_poissonian_noise = True, \n",
    "                                      algorithm = pipeline, \n",
    "                                      return_info = True, \n",
    "                                      output_dimension = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33c05ddd2f6d448a99507866ef9a9bc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Label(value='Decomposition component index', layout=Layout(width='15%')), IntSli…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xrf_stack.plot_decomposition_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa_load = xrf_stack.get_decomposition_loadings()\n",
    "#fa_load.save('snv_eds_analysis/plage_TD_eds_FA_4_load')\n",
    "\n",
    "fa_fact = xrf_stack.get_decomposition_factors()\n",
    "#fa_fact.set_elements(elements)\n",
    "\n",
    "#fa_fact.save('snv_eds_analysis/plage_TD_eds_FA_4_fact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa_load_array = fa_load.data\n",
    "fa_fact_array = fa_fact.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 201, 201), (3, 4096))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fa_load_array.shape, fa_fact_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa_facts, ydim, xdim = fa_load.data.shape\n",
    "\n",
    "fact_load_vect = pd.DataFrame((fa_load.data.reshape(fa_facts, ydim * xdim).T), columns = ['Factor 1','Factor 2','Factor 3','Factor 4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_load_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa_facts, ydim,xdim = fa_load.data.shape\n",
    "fa_vect = fa_load.data.reshape(fa_facts,ydim*xdim).transpose()\n",
    "fa_vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings_to_cluster = fa_vect[:,:fa_facts]\n",
    "loadings_to_cluster.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate the number of rows and columns for the plot grid\n",
    "if len(fa_load_array.shape) == 0:\n",
    "    num_fac = 1\n",
    "else:\n",
    "    num_fac = fa_load_array.shape[0] - 1\n",
    "num_cols = 3\n",
    "num_rows = int(np.ceil(num_fac / num_cols))\n",
    "\n",
    "# Create a figure and a grid of subplots\n",
    "fig, axs = plt.subplots(num_rows, num_cols, figsize=(10, 3*num_rows))\n",
    "\n",
    "# plots the data on subplots\n",
    "for i in range(num_fac):\n",
    "    row = i // num_cols\n",
    "    col = i % num_cols\n",
    "    axs[row, col].scatter(fa_load_array[:, 0], fa_load_array[:, i+1], s=5)\n",
    "    axs[row, col].set_xlabel(\"Factor loading 1\")\n",
    "    axs[row, col].set_ylabel(\"Factor loading {}\".format(i+2))\n",
    "\n",
    "# removes subplots without data\n",
    "for i in range(num_fac, num_rows*num_cols):\n",
    "    axs.flat[i].remove()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# saves figure\n",
    "plt.savefig('fa.png', dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[101], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m row \u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_cols\n\u001b[1;32m     16\u001b[0m col \u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m%\u001b[39m num_cols\n\u001b[0;32m---> 17\u001b[0m \u001b[43maxs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mscatter(fa_fact_array[:, \u001b[38;5;241m0\u001b[39m], fa_fact_array[:, i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m], s\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     18\u001b[0m axs[row, col]\u001b[38;5;241m.\u001b[39mset_xlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFactor loading 1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m axs[row, col]\u001b[38;5;241m.\u001b[39mset_ylabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFactor loading \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m2\u001b[39m))\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "# number of rows and columns for subplot grid\n",
    "num_fac = fa_fact_array.shape[0] - 1\n",
    "num_cols = 3\n",
    "num_rows = int(np.ceil(num_fac / num_cols))\n",
    "\n",
    "# figure; grid of subplots\n",
    "fig, axs = plt.subplots(num_rows, num_cols, figsize=(15, 5*num_rows))\n",
    "\n",
    "# plots the data on subplots\n",
    "for i in range(num_fac):\n",
    "    row = i // num_cols\n",
    "    col = i % num_cols\n",
    "    axs[row, col].scatter(fa_fact_array[:, 0], fa_fact_array[:, i+1])\n",
    "    axs[row, col].set_xlabel(\"Factor 1\")\n",
    "    axs[row, col].set_ylabel(\"Factor {}\".format(i+2))\n",
    "\n",
    "# removes subplots without data\n",
    "for i in range(num_fac, num_rows*num_cols):\n",
    "    axs.flat[i].remove()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# saves figure\n",
    "plt.savefig('fa.png', dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ICA on PCA loadings\n",
    "blind source separation (BSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses PCA results for ICA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "using fuzzy c-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GKProbabilistic(Probabilistic, GustafsonKesselMixin):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clus = sig_components\n",
    "clusterer = GKProbabilistic(n_clusters=num_clus, n_init=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mclusterer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43msig_loadings_2d\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/melt_map_mla/lib/python3.9/site-packages/skcmeans/algorithms.py:372\u001b[0m, in \u001b[0;36mGustafsonKesselMixin.fit\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    360\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimizes cluster centers by restarting convergence several times.\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \n\u001b[1;32m    362\u001b[0m \u001b[38;5;124;03m    Extends the default behaviour by recalculating the covariance matrix\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    370\u001b[0m \n\u001b[1;32m    371\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 372\u001b[0m     j_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mGustafsonKesselMixin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovariance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculate_covariance(x)\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m j_list\n",
      "File \u001b[0;32m~/melt_map_mla/lib/python3.9/site-packages/skcmeans/algorithms.py:125\u001b[0m, in \u001b[0;36mCMeans.fit\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcenters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemberships \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m objective \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective(x)\n\u001b[1;32m    127\u001b[0m j_list\u001b[38;5;241m.\u001b[39mappend(objective)\n",
      "File \u001b[0;32m~/melt_map_mla/lib/python3.9/site-packages/skcmeans/algorithms.py:153\u001b[0m, in \u001b[0;36mCMeans.converge\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter):\n\u001b[1;32m    152\u001b[0m     j_old \u001b[38;5;241m=\u001b[39m j_new\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m     centers\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcenters)\n\u001b[1;32m    155\u001b[0m     j_new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective(x)\n",
      "File \u001b[0;32m~/melt_map_mla/lib/python3.9/site-packages/skcmeans/algorithms.py:392\u001b[0m, in \u001b[0;36mGustafsonKesselMixin.update\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcenters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculate_centers(x)\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovariance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculate_covariance(x)\n\u001b[0;32m--> 392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemberships \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_memberships\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/melt_map_mla/lib/python3.9/site-packages/skcmeans/algorithms.py:276\u001b[0m, in \u001b[0;36mProbabilistic.calculate_memberships\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_memberships\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 276\u001b[0m     distances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistances\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m     distances[distances \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-18\u001b[39m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39mpower(\n\u001b[1;32m    279\u001b[0m         np\u001b[38;5;241m.\u001b[39mdivide(distances[:, :, np\u001b[38;5;241m.\u001b[39mnewaxis], distances[:, np\u001b[38;5;241m.\u001b[39mnewaxis, :]),\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/melt_map_mla/lib/python3.9/site-packages/skcmeans/algorithms.py:399\u001b[0m, in \u001b[0;36mGustafsonKesselMixin.distances\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    395\u001b[0m covariance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovariance \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovariance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \\\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculate_covariance(x)\n\u001b[1;32m    397\u001b[0m d \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcenters[:, np\u001b[38;5;241m.\u001b[39mnewaxis]\n\u001b[1;32m    398\u001b[0m left_multiplier \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m--> 399\u001b[0m     np\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m...ij,...jk\u001b[39m\u001b[38;5;124m'\u001b[39m, d, \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcovariance\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39msum(left_multiplier \u001b[38;5;241m*\u001b[39m d, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mT\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36minv\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/melt_map_mla/lib/python3.9/site-packages/numpy/linalg/linalg.py:552\u001b[0m, in \u001b[0;36minv\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m    550\u001b[0m signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD->D\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md->d\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    551\u001b[0m extobj \u001b[38;5;241m=\u001b[39m get_linalg_error_extobj(_raise_linalgerror_singular)\n\u001b[0;32m--> 552\u001b[0m ainv \u001b[38;5;241m=\u001b[39m \u001b[43m_umath_linalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap(ainv\u001b[38;5;241m.\u001b[39mastype(result_t, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "File \u001b[0;32m~/melt_map_mla/lib/python3.9/site-packages/numpy/linalg/linalg.py:89\u001b[0m, in \u001b[0;36m_raise_linalgerror_singular\u001b[0;34m(err, flag)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_raise_linalgerror_singular\u001b[39m(err, flag):\n\u001b[0;32m---> 89\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSingular matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "clusterer.fit(sig_loadings_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pgk \u001b[38;5;241m=\u001b[39m \u001b[43mPGK\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_clusters\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_clus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43msig_loadings_2d\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Process results for visualisation\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(pgk\u001b[38;5;241m.\u001b[39mmemberships_)\n",
      "File \u001b[0;32m~/melt_map_mla/lib/python3.9/site-packages/skcmeans/algorithms.py:372\u001b[0m, in \u001b[0;36mGustafsonKesselMixin.fit\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    360\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimizes cluster centers by restarting convergence several times.\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \n\u001b[1;32m    362\u001b[0m \u001b[38;5;124;03m    Extends the default behaviour by recalculating the covariance matrix\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    370\u001b[0m \n\u001b[1;32m    371\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 372\u001b[0m     j_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mGustafsonKesselMixin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovariance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculate_covariance(x)\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m j_list\n",
      "File \u001b[0;32m~/melt_map_mla/lib/python3.9/site-packages/skcmeans/algorithms.py:125\u001b[0m, in \u001b[0;36mCMeans.fit\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcenters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemberships \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m objective \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective(x)\n\u001b[1;32m    127\u001b[0m j_list\u001b[38;5;241m.\u001b[39mappend(objective)\n",
      "File \u001b[0;32m~/melt_map_mla/lib/python3.9/site-packages/skcmeans/algorithms.py:153\u001b[0m, in \u001b[0;36mCMeans.converge\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter):\n\u001b[1;32m    152\u001b[0m     j_old \u001b[38;5;241m=\u001b[39m j_new\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m     centers\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcenters)\n\u001b[1;32m    155\u001b[0m     j_new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective(x)\n",
      "File \u001b[0;32m~/melt_map_mla/lib/python3.9/site-packages/skcmeans/algorithms.py:392\u001b[0m, in \u001b[0;36mGustafsonKesselMixin.update\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcenters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculate_centers(x)\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovariance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculate_covariance(x)\n\u001b[0;32m--> 392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemberships \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_memberships\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/melt_map_mla/lib/python3.9/site-packages/skcmeans/algorithms.py:276\u001b[0m, in \u001b[0;36mProbabilistic.calculate_memberships\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_memberships\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 276\u001b[0m     distances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistances\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m     distances[distances \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-18\u001b[39m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39mpower(\n\u001b[1;32m    279\u001b[0m         np\u001b[38;5;241m.\u001b[39mdivide(distances[:, :, np\u001b[38;5;241m.\u001b[39mnewaxis], distances[:, np\u001b[38;5;241m.\u001b[39mnewaxis, :]),\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/melt_map_mla/lib/python3.9/site-packages/skcmeans/algorithms.py:399\u001b[0m, in \u001b[0;36mGustafsonKesselMixin.distances\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    395\u001b[0m covariance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovariance \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovariance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \\\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculate_covariance(x)\n\u001b[1;32m    397\u001b[0m d \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcenters[:, np\u001b[38;5;241m.\u001b[39mnewaxis]\n\u001b[1;32m    398\u001b[0m left_multiplier \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m--> 399\u001b[0m     np\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m...ij,...jk\u001b[39m\u001b[38;5;124m'\u001b[39m, d, \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcovariance\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39msum(left_multiplier \u001b[38;5;241m*\u001b[39m d, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mT\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36minv\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/melt_map_mla/lib/python3.9/site-packages/numpy/linalg/linalg.py:552\u001b[0m, in \u001b[0;36minv\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m    550\u001b[0m signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD->D\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md->d\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    551\u001b[0m extobj \u001b[38;5;241m=\u001b[39m get_linalg_error_extobj(_raise_linalgerror_singular)\n\u001b[0;32m--> 552\u001b[0m ainv \u001b[38;5;241m=\u001b[39m \u001b[43m_umath_linalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap(ainv\u001b[38;5;241m.\u001b[39mastype(result_t, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "File \u001b[0;32m~/melt_map_mla/lib/python3.9/site-packages/numpy/linalg/linalg.py:89\u001b[0m, in \u001b[0;36m_raise_linalgerror_singular\u001b[0;34m(err, flag)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_raise_linalgerror_singular\u001b[39m(err, flag):\n\u001b[0;32m---> 89\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSingular matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "pgk = PGK(n_clusters =num_clus, n_init=20).fit(sig_loadings_2d)\n",
    "# Process results for visualisation\n",
    "print(pgk.memberships_)\n",
    "labels_ = np.argmax(pgk.memberships_, axis=1)\n",
    "memberships_ = pgk.memberships_[range(len(pgk.memberships_)), labels_] \n",
    "\n",
    "labels = labels_.reshape([ydim,xdim])\n",
    "labels=hs.signals.Signal2D(labels)\n",
    "labels.plot(cmap='tab10')\n",
    "\n",
    "mems=pgk.memberships_.reshape(ydim,xdim,num_clus)\n",
    "print(mems.shape)\n",
    "\n",
    "mem_maps=hs.signals.Signal2D(mems)\n",
    "mem_maps=mem_maps.transpose(signal_axes=(2,0))\n",
    "mem_maps.change_dtype('float32')\n",
    "mem_maps.plot(cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'k_range' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# run c-means clustering for each value of k\u001b[39;00m\n\u001b[1;32m      2\u001b[0m wcss \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[43mk_range\u001b[49m:\n\u001b[1;32m      4\u001b[0m     cmeans \u001b[38;5;241m=\u001b[39m Probabilistic(n_clusters\u001b[38;5;241m=\u001b[39mk, max_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      5\u001b[0m     cmeans\u001b[38;5;241m.\u001b[39mfit(sig_loadings_2d)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'k_range' is not defined"
     ]
    }
   ],
   "source": [
    "# run c-means clustering for each value of k\n",
    "wcss = []\n",
    "for k in k_range:\n",
    "    cmeans = Probabilistic(n_clusters=k, max_iterations=1000, random_state=0)\n",
    "    cmeans.fit(sig_loadings_2d)\n",
    "    u, u0, d, jm, p, fpc = cmeans.cmeans_predict(sig_loadings_2d, return_probabilities=True)\n",
    "    wcss.append(np.sum((sig_loadings_2d - np.dot(u, cmeans.cluster_centers_))**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the WCSS as a function of k\n",
    "plt.plot(k_range, wcss)\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.title('Elbow method for optimal number of clusters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clus=bic\n",
    "pgk = PGK(n_clusters =num_clus, n_init=10).fit(loadings_to_cluster)\n",
    "# Process results for visualisation\n",
    "print(pgk.memberships_)\n",
    "labels_ = np.argmax(pgk.memberships_, axis=1)\n",
    "memberships_ = pgk.memberships_[range(len(pgk.memberships_)), labels_] \n",
    "\n",
    "labels = labels_.reshape([ydim,xdim])\n",
    "labels=hs.signals.Signal2D(labels)\n",
    "labels.plot(cmap='tab10')\n",
    "\n",
    "mems=pgk.memberships_.reshape(ydim,xdim,num_clus)\n",
    "print(mems.shape)\n",
    "\n",
    "mem_maps=hs.signals.Signal2D(mems)\n",
    "mem_maps=mem_maps.transpose(signal_axes=(2,0))\n",
    "mem_maps.change_dtype('float32')\n",
    "mem_maps.plot(cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clus=bic\n",
    "pgk = PGK(n_clusters =num_clus, n_init=10).fit(loadings_to_cluster)\n",
    "# Process results for visualisation\n",
    "print(pgk.memberships_)\n",
    "labels_ = np.argmax(pgk.memberships_, axis=1)\n",
    "memberships_ = pgk.memberships_[range(len(pgk.memberships_)), labels_] \n",
    "\n",
    "labels = labels_.reshape([ydim,xdim])\n",
    "labels=hs.signals.Signal2D(labels)\n",
    "labels.plot(cmap='tab10')\n",
    "\n",
    "mems=pgk.memberships_.reshape(ydim,xdim,num_clus)\n",
    "print(mems.shape)\n",
    "\n",
    "mem_maps=hs.signals.Signal2D(mems)\n",
    "mem_maps=mem_maps.transpose(signal_axes=(2,0))\n",
    "mem_maps.change_dtype('float32')\n",
    "mem_maps.plot(cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal sums from regions of interest\n",
    "selected from the dimension reduction and the clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# working..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensions of the map\n",
    "also sets save paths..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrf_map.change_dtype('float32')\n",
    "#xrf_map.save(save_path+'map1')\n",
    "\n",
    "xrf_stack = xrf_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Signal1D, title: , dimensions: (201, 201|4096)>"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xrf_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrf_stack.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwrite '/Users/joshuashea/melt_mapsat16_map2_002_mapped_crop.hspy' (y/n)?\n",
      "y\n"
     ]
    }
   ],
   "source": [
    "xrf_stack.change_dtype('float32')\n",
    "\n",
    "\n",
    "xrf_stack.save(save_path+'at16_map2_002_mapped_crop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposition info:\n",
      "  normalize_poissonian_noise=False\n",
      "  algorithm=sklearn_pca\n",
      "  output_dimension=20\n",
      "  centre=None\n",
      "scikit-learn estimator:\n",
      "PCA(n_components=20)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04bbfc07b5f943f3b9530220f44fa788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Label(value='Decomposition component index', layout=Layout(width='15%')), IntSli…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "xrf_stack.decomposition(normalize_poissonian_noise=False, algorithm=\"sklearn_pca\", output_dimension=20)\n",
    "\n",
    "xrf_stack.plot_explained_variance_ratio(log=True, vline=True)\n",
    "\n",
    "xrf_stack.plot_decomposition_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Principal component', ylabel='Cumulative explained variance ratio'>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xrf_stack.plot_cumulative_explained_variance_ratio()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**note** iterative cropping above showed that most data/ analysis was better when cropping down to the shape (157, 63|2001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrf_stack.save(save_path+'lisheen_low_res_map_data_Crop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrf_stack = xrf_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrf_stack.decomposition(normalize_poissonian_noise=True, algorithm='PCA', output_dimension=15)\n",
    "xrf_stack.plot_cumulative_explained_variance_ratio()\n",
    "xrf_stack.plot_explained_variance_ratio(log=True, vline=True)\n",
    "xrf_stack.plot_decomposition_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bic_elbow(signal, decomposition_method='sklearn_pca', n_components_range=np.arange(2, 11)):\n",
    "    \"\"\"\n",
    "    Perform a BIC elbow test to find the optimal number of components for a signal decomposition method.\n",
    "    Parameters\n",
    "    ----------\n",
    "    signal : `hyperspy.signals.Signal2D` or `hyperspy.signals.SignalND`\n",
    "        The hyperspy signal to perform the BIC elbow test on.\n",
    "    decomposition_method : str, optional\n",
    "        The signal decomposition method to use. Default is 'PCA'.\n",
    "    n_components_range : array-like, optional\n",
    "        The range of possible numbers of components to fit the signal model for.\n",
    "        Default is np.arange(2, 11).\n",
    "    Returns\n",
    "    -------\n",
    "    optimal_n_components : int\n",
    "        The optimal number of components that minimizes the BIC score.\n",
    "    Notes\n",
    "    -----\n",
    "    This function uses the Bayesian Information Criterion (BIC) to select the optimal number of components\n",
    "    for the given signal decomposition method. The BIC score is computed for each number of components\n",
    "    in the range, and the elbow point in the BIC curve is identified as the optimal number of components.\n",
    "    \"\"\"\n",
    "    bic_scores = []\n",
    "    for n_components in n_components_range:\n",
    "        # Fit the signal model\n",
    "        if decomposition_method == 'sklearn_pca':\n",
    "            model = signal.decomposition.PCA(n_components=n_components)\n",
    "        elif decomposition_method == 'NMF':\n",
    "            model = signal.decomposition.nmf(n_components=n_components)\n",
    "        elif decomposition_method == 'ICA':\n",
    "            model = signal.decomposition.ica(n_components=n_components)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown decomposition method: {decomposition_method}\")\n",
    "\n",
    "        # Compute the log-likelihood and number of model parameters\n",
    "        log_likelihood = model.score(signal)\n",
    "        n_samples, n_features = signal.data.shape\n",
    "        n_params = n_components * (n_features + 1)\n",
    "\n",
    "        # Compute the BIC score\n",
    "        bic = -2 * log_likelihood + n_params * np.log(n_samples)\n",
    "        bic_scores.append(bic)\n",
    "\n",
    "    # Plot the BIC scores as a function of the number of components\n",
    "    plt.plot(n_components_range, bic_scores, 'o-')\n",
    "    plt.xlabel('Number of components')\n",
    "    plt.ylabel('BIC score')\n",
    "    plt.title(f'BIC elbow test for {decomposition_method} decomposition')\n",
    "    plt.show()\n",
    "\n",
    "    # Find the elbow point\n",
    "    diff = np.diff(bic_scores)\n",
    "    elbow_index = np.argmax(diff) + 1\n",
    "    optimal_n_components = n_components_range[elbow_index]\n",
    "    print(f'The optimal number of components is {optimal_n_components}')\n",
    "\n",
    "    return optimal_n_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'PCA'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbic_elbow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxrf_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecomposition_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msklearn_pca\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_components_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[32], line 27\u001b[0m, in \u001b[0;36mbic_elbow\u001b[0;34m(signal, decomposition_method, n_components_range)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n_components \u001b[38;5;129;01min\u001b[39;00m n_components_range:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# Fit the signal model\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m decomposition_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msklearn_pca\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 27\u001b[0m         model \u001b[38;5;241m=\u001b[39m \u001b[43msignal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecomposition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPCA\u001b[49m(n_components\u001b[38;5;241m=\u001b[39mn_components)\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m decomposition_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNMF\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     29\u001b[0m         model \u001b[38;5;241m=\u001b[39m signal\u001b[38;5;241m.\u001b[39mdecomposition\u001b[38;5;241m.\u001b[39mnmf(n_components\u001b[38;5;241m=\u001b[39mn_components)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'PCA'"
     ]
    }
   ],
   "source": [
    "bic_elbow(xrf_map, decomposition_method='sklearn_pca', n_components_range=np.arange(2, 11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown decomposition method: PCA",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m     xrf_data \u001b[38;5;241m=\u001b[39m h5[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/xrmmap/mcasum/counts\u001b[39m\u001b[38;5;124m'\u001b[39m][:]\n\u001b[1;32m      4\u001b[0m xrf_map \u001b[38;5;241m=\u001b[39m hs\u001b[38;5;241m.\u001b[39msignals\u001b[38;5;241m.\u001b[39mSignal1D(xrf_data)\n\u001b[0;32m----> 6\u001b[0m optimal_n_components \u001b[38;5;241m=\u001b[39m \u001b[43mbic_elbow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxrf_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecomposition_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPCA\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_components_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 33\u001b[0m, in \u001b[0;36mbic_elbow\u001b[0;34m(signal, decomposition_method, n_components_range)\u001b[0m\n\u001b[1;32m     31\u001b[0m     model \u001b[38;5;241m=\u001b[39m signal\u001b[38;5;241m.\u001b[39mdecomposition\u001b[38;5;241m.\u001b[39mica(n_components\u001b[38;5;241m=\u001b[39mn_components)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown decomposition method: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdecomposition_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Compute the log-likelihood and number of model parameters\u001b[39;00m\n\u001b[1;32m     36\u001b[0m log_likelihood \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mscore(signal)\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown decomposition method: PCA"
     ]
    }
   ],
   "source": [
    "with h5py.File(data, 'r') as h5:\n",
    "    xrf_data = h5['/xrmmap/mcasum/counts'][:]\n",
    "\n",
    "xrf_map = hs.signals.Signal1D(xrf_data)\n",
    "\n",
    "optimal_n_components = bic_elbow(xrf_map, decomposition_method='PCA', n_components_range=np.arange(2, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## re-explore\n",
    "\n",
    "get 6 good factors... need to label peaks etc but starting to pull out data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joshuashea/xraylarch/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:289: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposition info:\n",
      "  normalize_poissonian_noise=False\n",
      "  algorithm=NMF\n",
      "  output_dimension=5\n",
      "  centre=None\n",
      "scikit-learn estimator:\n",
      "NMF(n_components=5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joshuashea/xraylarch/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1637: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcdb581e536645a5b27c857af180b408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Label(value='Decomposition component index', layout=Layout(width='15%')), IntSli…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xrf_stack.decomposition(normalize_poissonian_noise=False, algorithm='NMF', output_dimension=5)\n",
    "\n",
    "\n",
    "xrf_stack.plot_decomposition_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrf_stack.decomposition(normalize_poissonian_noise=True, algorithm='NMF', output_dimension=5)\n",
    "\n",
    "\n",
    "xrf_stack.plot_decomposition_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## examine with factor analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposition info:\n",
      "  normalize_poissonian_noise=False\n",
      "  algorithm=Pipeline(steps=[('FA', FactorAnalysis(n_components=5, rotation='varimax'))])\n",
      "  output_dimension=11\n",
      "  centre=None\n",
      "scikit-learn estimator:\n",
      "Pipeline(steps=[('FA', FactorAnalysis(n_components=5, rotation='varimax'))])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4941caacdc514b1db784b6f60801988a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Label(value='Decomposition component index', layout=Layout(width='15%')), IntSli…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline = Pipeline([(\"FA\", FactorAnalysis(n_components=5,rotation = 'varimax'))])\n",
    "\n",
    "xrf_stack.decomposition(normalize_poissonian_noise=False, algorithm=pipeline, return_info=True,output_dimension=11)\n",
    "\n",
    "xrf_stack.plot_decomposition_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Label(value='Decomposition component index', layout=Layout(width='15%')), IntSli…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xrf_stack.plot_decomposition_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa_load= xrf_stack.get_decomposition_loadings()\n",
    "#fa_load.save('snv_eds_analysis/plage_TD_eds_FA_4_load')\n",
    "\n",
    "fa_fact= xrf_stack.get_decomposition_factors()\n",
    "#fa_fact.set_elements(elements)\n",
    "\n",
    "\n",
    "#fa_fact.save('snv_eds_analysis/plage_TD_eds_FA_4_fact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa_facts, ydim,xdim=fa_load.data.shape\n",
    "\n",
    "fact_load_vect= pd.DataFrame((fa_load.data.reshape(fa_facts, ydim*xdim).T), columns = ['Factor 1','Factor 2','Factor 3','Factor 4'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fact_load_vect' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [126], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfact_load_vect\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fact_load_vect' is not defined"
     ]
    }
   ],
   "source": [
    "fact_load_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa_facts, ydim,xdim=fa_load.data.shape\n",
    "fa_vect=fa_load.data.reshape(fa_facts,ydim*xdim).transpose()\n",
    "fa_vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings_to_cluster=fa_vect[:,:fa_facts]\n",
    "loadings_to_cluster.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Probabilistic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mPGK\u001b[39;00m(\u001b[43mProbabilistic\u001b[49m, GustafsonKesselMixin):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Probabilistic' is not defined"
     ]
    }
   ],
   "source": [
    "class PGK(Probabilistic, GustafsonKesselMixin):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clus=bic\n",
    "pgk = PGK(n_clusters =num_clus, n_init=10).fit(loadings_to_cluster)\n",
    "# Process results for visualisation\n",
    "print(pgk.memberships_)\n",
    "labels_ = np.argmax(pgk.memberships_, axis=1)\n",
    "memberships_ = pgk.memberships_[range(len(pgk.memberships_)), labels_] \n",
    "\n",
    "labels = labels_.reshape([ydim,xdim])\n",
    "labels=hs.signals.Signal2D(labels)\n",
    "labels.plot(cmap='tab10')\n",
    "\n",
    "mems=pgk.memberships_.reshape(ydim,xdim,num_clus)\n",
    "print(mems.shape)\n",
    "\n",
    "mem_maps=hs.signals.Signal2D(mems)\n",
    "mem_maps=mem_maps.transpose(signal_axes=(2,0))\n",
    "mem_maps.change_dtype('float32')\n",
    "mem_maps.plot(cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "\n",
    "mem_maps.plot(cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## next steps\n",
    "\n",
    "1. makes sense to calcualte an elbow test or examine the BIC to determine the 'optimal' number of clusters\n",
    "2. explore applying HDBSCAN on the FA/ PCA factors\n",
    "3. would also to explore the UMAP pretreatment then HDBSCAN\n",
    "4. maybe blind source separation (ie ICA) after PCA.\n",
    "\n",
    "for 2 and 3 need a different virtual enviroment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
