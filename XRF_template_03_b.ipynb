{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt5\n",
    "\n",
    "# handling XRF HDF5 data\n",
    "import h5py \n",
    "import hyperspy.api as hs\n",
    "\n",
    "#handling data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# data vis\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import glob\n",
    "\n",
    "# data processing, dimension reduction and clustering\n",
    "from sklearn.decomposition import PCA, FactorAnalysis\n",
    "\n",
    "\n",
    "import scipy\n",
    "import umap\n",
    "import hdbscan\n",
    "import sklearn as skl\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "import sklearn.cluster as cluster\n",
    "#from skcmeans.algorithms import Probabilistic\n",
    "#from skcmeans.algorithms import Probabilistic, Possibilistic, GustafsonKesselMixin\n",
    "# from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert XRF data to Hyperspy standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mISE_500sqaures_A21-016_Map1_001.h5\u001b[m\u001b[m\n",
      "\u001b[31mISE_500sqaures_A21_054_botom_right_map_center_001.h5\u001b[m\u001b[m\n",
      "map.hspy\n",
      "map1.hspy\n",
      "map<class 'map'>.hspy\n",
      "/Users/user/Documents/github/melt_maps\n"
     ]
    }
   ],
   "source": [
    "!cd '/Users/user/Documents/Projects/XRF_machine_learning/data'\n",
    "!ls '/Users/user/Documents/Projects/XRF_machine_learning/data'\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = '/Users/user/Documents/Projects/XRF_machine_learning/data/ISE_500sqaures_A21_054_botom_right_map_center_001.h5'\n",
    "save_path = '/Users/user/Documents/Projects/XRF_machine_learning/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_and_list_h5_objects(obj, level=0, path='', dataset_list=[]):\n",
    "    \"\"\"\n",
    "   checks whether the input object is a file or group, prints its name/type \n",
    "   accordingly, and then goes through its subgroups and datasets, printing \n",
    "   their names/types and adding the dataset paths to the list 'dataset_list'.\n",
    "   \n",
    "   Returns\n",
    "    -------\n",
    "    prints the structure of the HDF5 file and returns a list of dataset paths\n",
    "    \n",
    "   Note\n",
    "    -----\n",
    "    This function makes it easier to look through the HDF5 blackbox and call \n",
    "    datasets.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, h5py.File):\n",
    "        print(\"File name: {}\".format(obj.filename))\n",
    "        obj = obj['/']\n",
    "    elif isinstance(obj, h5py.Group):\n",
    "        print(\"Group: {}\".format(obj.name))\n",
    "\n",
    "    for key, val in obj.items():\n",
    "        new_path = path + '/' + key if path else key\n",
    "        if isinstance(val, h5py.Group):\n",
    "            print(\"  \" * level + \"Group: {}\".format(key))\n",
    "            print_and_list_h5_objects(val, level + 1, new_path, dataset_list)\n",
    "        else:\n",
    "            print(\"  \" * level + \"Dataset: {}\".format(key))\n",
    "            dataset_list.append(new_path)\n",
    "    \n",
    "    return dataset_list\n",
    "\n",
    "# USE EXAMPLE\n",
    "# with h5py.File(data,'r') as h5:\n",
    "#    datasets = print_and_list_h5_objects(h5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File name: /Users/user/Documents/Projects/XRF_machine_learning/data/ISE_500sqaures_A21_054_botom_right_map_center_001.h5\n",
      "Group: xrmmap\n",
      "Group: /xrmmap\n",
      "  Group: areas\n",
      "Group: /xrmmap/areas\n",
      "    Dataset: A21-054_Br_Xanes_spot\n",
      "    Dataset: A21-054_Br_Xanes_spot_2\n",
      "    Dataset: A21-054_I_Xanes_spot\n",
      "    Dataset: A21-054_I_Xanes_spot_2\n",
      "    Dataset: area_003\n",
      "    Dataset: area_004\n",
      "  Group: config\n",
      "Group: /xrmmap/config\n",
      "    Group: environ\n",
      "Group: /xrmmap/config/environ\n",
      "      Dataset: address\n",
      "      Dataset: name\n",
      "      Dataset: value\n",
      "    Group: general\n",
      "Group: /xrmmap/config/general\n",
      "      Dataset: basedir\n",
      "      Dataset: envfile\n",
      "    Group: mca_calib\n",
      "Group: /xrmmap/config/mca_calib\n",
      "      Dataset: offset\n",
      "      Dataset: quad\n",
      "      Dataset: slope\n",
      "    Group: mca_settings\n",
      "Group: /xrmmap/config/mca_settings\n",
      "    Group: motor_controller\n",
      "Group: /xrmmap/config/motor_controller\n",
      "      Dataset: group\n",
      "      Dataset: host\n",
      "      Dataset: mode\n",
      "      Dataset: passwd\n",
      "      Dataset: positioners\n",
      "      Dataset: type\n",
      "      Dataset: user\n",
      "    Group: notes\n",
      "Group: /xrmmap/config/notes\n",
      "    Group: positioners\n",
      "Group: /xrmmap/config/positioners\n",
      "      Dataset: 13IDE:En:Energy\n",
      "      Dataset: 13IDE:m19\n",
      "      Dataset: 13IDE:m25\n",
      "      Dataset: 13IDE:m28\n",
      "      Dataset: 13IDE:m31\n",
      "      Dataset: 13IDE:m32\n",
      "      Dataset: 13IDE:m34\n",
      "      Dataset: 13IDE:m35\n",
      "      Dataset: 13IDE:m36\n",
      "      Dataset: 13IDE:m39\n",
      "      Dataset: 13IDE:m4\n",
      "      Dataset: 13IDE:m40\n",
      "      Dataset: 13IDE:userTran5.A\n",
      "      Dataset: 13XRM:ANA:Energy\n",
      "      Dataset: 13XRM:m1\n",
      "      Dataset: 13XRM:m10\n",
      "      Dataset: 13XRM:m11\n",
      "      Dataset: 13XRM:m2\n",
      "      Dataset: 13XRM:m3\n",
      "      Dataset: 13XRM:m4\n",
      "      Dataset: 13XRM:m5\n",
      "      Dataset: 13XRM:m6\n",
      "    Group: rois\n",
      "Group: /xrmmap/config/rois\n",
      "      Dataset: address\n",
      "      Dataset: limits\n",
      "      Dataset: name\n",
      "    Group: scan\n",
      "Group: /xrmmap/config/scan\n",
      "      Dataset: comments\n",
      "      Dataset: dimension\n",
      "      Dataset: filename\n",
      "      Dataset: pos1\n",
      "      Dataset: pos2\n",
      "      Dataset: start1\n",
      "      Dataset: start2\n",
      "      Dataset: step1\n",
      "      Dataset: step2\n",
      "      Dataset: stop1\n",
      "      Dataset: stop2\n",
      "      Dataset: text\n",
      "      Dataset: time1\n",
      "  Group: mcasum\n",
      "Group: /xrmmap/mcasum\n",
      "    Dataset: counts\n",
      "    Dataset: dtfactor\n",
      "    Dataset: energy\n",
      "    Dataset: inpcounts\n",
      "    Dataset: livetime\n",
      "    Dataset: outcounts\n",
      "    Dataset: realtime\n",
      "  Group: positions\n",
      "Group: /xrmmap/positions\n",
      "    Dataset: address\n",
      "    Dataset: name\n",
      "    Dataset: pos\n",
      "  Group: roimap\n",
      "Group: /xrmmap/roimap\n",
      "    Dataset: det_address\n",
      "    Dataset: det_cor\n",
      "    Dataset: det_name\n",
      "    Dataset: det_raw\n",
      "    Group: mcasum\n",
      "Group: /xrmmap/roimap/mcasum\n",
      "      Group: As Ka\n",
      "Group: /xrmmap/roimap/mcasum/As Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Ba La\n",
      "Group: /xrmmap/roimap/mcasum/Ba La\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Br Ka\n",
      "Group: /xrmmap/roimap/mcasum/Br Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Ca Ka\n",
      "Group: /xrmmap/roimap/mcasum/Ca Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Ce La\n",
      "Group: /xrmmap/roimap/mcasum/Ce La\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Cl Ka\n",
      "Group: /xrmmap/roimap/mcasum/Cl Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Co Ka\n",
      "Group: /xrmmap/roimap/mcasum/Co Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Cr Ka\n",
      "Group: /xrmmap/roimap/mcasum/Cr Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Cu Ka\n",
      "Group: /xrmmap/roimap/mcasum/Cu Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Eu La\n",
      "Group: /xrmmap/roimap/mcasum/Eu La\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Fe Ka\n",
      "Group: /xrmmap/roimap/mcasum/Fe Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Ge Ka\n",
      "Group: /xrmmap/roimap/mcasum/Ge Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: I La1\n",
      "Group: /xrmmap/roimap/mcasum/I La1\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: I Lb1\n",
      "Group: /xrmmap/roimap/mcasum/I Lb1\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: K Ka\n",
      "Group: /xrmmap/roimap/mcasum/K Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Mn Ka\n",
      "Group: /xrmmap/roimap/mcasum/Mn Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Mo Ka\n",
      "Group: /xrmmap/roimap/mcasum/Mo Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Ni Ka\n",
      "Group: /xrmmap/roimap/mcasum/Ni Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: OutputCounts\n",
      "Group: /xrmmap/roimap/mcasum/OutputCounts\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: P Ka\n",
      "Group: /xrmmap/roimap/mcasum/P Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Pb La1\n",
      "Group: /xrmmap/roimap/mcasum/Pb La1\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Rb Ka\n",
      "Group: /xrmmap/roimap/mcasum/Rb Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Re La1\n",
      "Group: /xrmmap/roimap/mcasum/Re La1\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: S Ka\n",
      "Group: /xrmmap/roimap/mcasum/S Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Sc Ka\n",
      "Group: /xrmmap/roimap/mcasum/Sc Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Se Ka\n",
      "Group: /xrmmap/roimap/mcasum/Se Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Si Ka\n",
      "Group: /xrmmap/roimap/mcasum/Si Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Sr Ka\n",
      "Group: /xrmmap/roimap/mcasum/Sr Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Ti Ka\n",
      "Group: /xrmmap/roimap/mcasum/Ti Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: V Ka\n",
      "Group: /xrmmap/roimap/mcasum/V Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: V Kb\n",
      "Group: /xrmmap/roimap/mcasum/V Kb\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Y Ka\n",
      "Group: /xrmmap/roimap/mcasum/Y Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Zn Ka\n",
      "Group: /xrmmap/roimap/mcasum/Zn Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Zr Ka\n",
      "Group: /xrmmap/roimap/mcasum/Zr Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Zr L\n",
      "Group: /xrmmap/roimap/mcasum/Zr L\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "    Dataset: sum_cor\n",
      "    Dataset: sum_list\n",
      "    Dataset: sum_name\n",
      "    Dataset: sum_raw\n",
      "  Group: scalars\n",
      "Group: /xrmmap/scalars\n",
      "    Dataset: I0\n",
      "    Dataset: I0_raw\n",
      "    Dataset: I1\n",
      "    Dataset: I1_raw\n",
      "    Dataset: I2\n",
      "    Dataset: I2_raw\n",
      "    Dataset: TSCALER\n",
      "    Dataset: TSCALER_raw\n",
      "  Group: work\n",
      "Group: /xrmmap/work\n",
      "  Group: xrd1d\n",
      "Group: /xrmmap/xrd1d\n"
     ]
    }
   ],
   "source": [
    "# prints the h5 file structure\n",
    "with h5py.File(data,'r') as h5:\n",
    "    datasets = print_and_list_h5_objects(h5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracts congiuration settings of the run\n",
    "needed to correctly scale axes and the scale channels to KeV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     0         0\n",
      "0                 Experiment.User_Name      Ezad\n",
      "1           Experiment.Proposal_Number     78722\n",
      "2        Experiment.Beam_Size__Nominal       2um\n",
      "3     Experiment.Monochromator_Crystal   Si(111)\n",
      "4   Experiment.Double_H_Mirror_Stripes   rhodium\n",
      "..                                 ...       ...\n",
      "60                 SampleStage.CoarseY  177.9370\n",
      "61                   SampleStage.FineX  -0.25500\n",
      "62                   SampleStage.FineY         0\n",
      "63                   SampleStage.Theta   90.0000\n",
      "64                    Undulator.Energy    13.717\n",
      "\n",
      "[65 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# extracts the congiuration settings of the scan and beamline\n",
    "with h5py.File(data, 'r') as h5:\n",
    "    dataset_df = pd.DataFrame(h5['/xrmmap/config/environ/name'][:])\n",
    "    value_df = pd.DataFrame(h5['/xrmmap/config/environ/value'][:])\n",
    "    # Merges configuration name and value dataframes\n",
    "    values_df = pd.concat([dataset_df, value_df], axis=1)\n",
    "    # Decodes byte strings to regular strings\n",
    "    values_df = values_df.apply(lambda x: x.str.decode('utf-8') if isinstance(x[0], bytes) else x, axis=0)\n",
    "print(values_df)\n",
    "# Save dataframe to CSV file\n",
    "#values_df.to_csv('values_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Axis management and scaling\n",
    "\n",
    "these are needed to scale the data and pixelscorrectly etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(201, 201, 4096)\n"
     ]
    }
   ],
   "source": [
    "# extracts xrf map and channels from the HDF5 file\n",
    "with h5py.File(data, 'r') as h5:\n",
    "    xrf_data = h5['/xrmmap/mcasum/counts'][:]\n",
    "    shape = h5['/xrmmap/mcasum/counts'].shape\n",
    "print(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts xrf map to hyperspy and scales data\n",
    "\n",
    "def convert_xrf_to_hyperspy(xrf_data):\n",
    "    # converts xrf map to hyperspy\n",
    "    xrf_map = hs.signals.Signal1D(xrf_data)\n",
    "\n",
    "    # scales the x, y and theta of the hyperspy data\n",
    "    xrf_map.axes_manager.navigation_axes[0].name = 'x'\n",
    "    xrf_map.axes_manager.navigation_axes[0].units = 'mm'\n",
    "    xrf_map.axes_manager.navigation_axes[0].scale = 0.025\n",
    "\n",
    "    xrf_map.axes_manager.navigation_axes[1].name = 'y'\n",
    "    xrf_map.axes_manager.navigation_axes[1].units = 'mm'\n",
    "    xrf_map.axes_manager.navigation_axes[1].scale = 0.025\n",
    "\n",
    "    xrf_map.axes_manager.signal_axes[0].name = 'Energy'\n",
    "    xrf_map.axes_manager.signal_axes[0].units = 'KeV'\n",
    "    xrf_map.axes_manager.signal_axes[0].scale = 13.717/4096\n",
    "    \n",
    "    return xrf_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Axes manager, axes: (201, 201|4096)>\n",
      "            Name |   size |  index |  offset |   scale |  units \n",
      "================ | ====== | ====== | ======= | ======= | ====== \n",
      "               x |    201 |      0 |       0 |   0.025 |     mm \n",
      "               y |    201 |      0 |       0 |   0.025 |     mm \n",
      "---------------- | ------ | ------ | ------- | ------- | ------ \n",
      "          Energy |   4096 |      0 |       0 |  0.0033 |    KeV \n"
     ]
    }
   ],
   "source": [
    "xrf_map = convert_xrf_to_hyperspy(xrf_data)\n",
    "print(xrf_map.axes_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwrite '/Users/user/Documents/Projects/XRF_machine_learning/data/map.hspy' (y/n)?\n",
      "y\n"
     ]
    }
   ],
   "source": [
    "xrf_map.save(save_path+'map.hspy'.format(map))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension reduction\n",
    "can start from here once the data has been pulled from the hdf5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 109.61 ms\n"
     ]
    }
   ],
   "source": [
    "xrf_stack = hs.load(save_path+'map.hspy', stack = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Axes manager, axes: (201, 201|4096)>\n",
      "            Name |   size |  index |  offset |   scale |  units \n",
      "================ | ====== | ====== | ======= | ======= | ====== \n",
      "               x |    201 |      0 |       0 |   0.025 |     mm \n",
      "               y |    201 |      0 |       0 |   0.025 |     mm \n",
      "---------------- | ------ | ------ | ------- | ------- | ------ \n",
      "          Energy |   4096 |      0 |       0 |  0.0033 |    KeV \n"
     ]
    }
   ],
   "source": [
    "print(xrf_map.axes_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bic_elbow(signal, decomposition_method='sklearn_pca', n_components_range=np.arange(2, 11)):\n",
    "    \"\"\"\n",
    "    Perform a BIC elbow test to find the optimal number of components for a signal decomposition method.\n",
    "    Parameters\n",
    "    ----------\n",
    "    signal : `hyperspy.signals.Signal2D` or `hyperspy.signals.SignalND`\n",
    "        The hyperspy signal to perform the BIC elbow test on.\n",
    "    decomposition_method : str, optional\n",
    "        The signal decomposition method to use. Default is 'PCA'.\n",
    "    n_components_range : array-like, optional\n",
    "        The range of possible numbers of components to fit the signal model for.\n",
    "        Default is np.arange(2, 11).\n",
    "    Returns\n",
    "    -------\n",
    "    optimal_n_components : int\n",
    "        The optimal number of components that minimizes the BIC score.\n",
    "    Notes\n",
    "    -----\n",
    "    This function uses the Bayesian Information Criterion (BIC) to select the optimal number of components\n",
    "    for the given signal decomposition method. The BIC score is computed for each number of components\n",
    "    in the range, and the elbow point in the BIC curve is identified as the optimal number of components.\n",
    "    \"\"\"\n",
    "    bic_scores = []\n",
    "    for n_components in n_components_range:\n",
    "        # Fit the signal model\n",
    "        if decomposition_method == 'sklearn_pca':\n",
    "            model = signal.decomposition.PCA(n_components=n_components)\n",
    "        elif decomposition_method == 'NMF':\n",
    "            model = signal.decomposition.nmf(n_components=n_components)\n",
    "        elif decomposition_method == 'ICA':\n",
    "            model = signal.decomposition.ica(n_components=n_components)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown decomposition method: {decomposition_method}\")\n",
    "\n",
    "        # Compute the log-likelihood and number of model parameters\n",
    "        log_likelihood = model.score(signal)\n",
    "        n_samples, n_features = signal.data.shape\n",
    "        n_params = n_components * (n_features + 1)\n",
    "\n",
    "        # Compute the BIC score\n",
    "        bic = -2 * log_likelihood + n_params * np.log(n_samples)\n",
    "        bic_scores.append(bic)\n",
    "\n",
    "    # Plot the BIC scores as a function of the number of components\n",
    "    plt.plot(n_components_range, bic_scores, 'o-')\n",
    "    plt.xlabel('Number of components')\n",
    "    plt.ylabel('BIC score')\n",
    "    plt.title(f'BIC elbow test for {decomposition_method} decomposition')\n",
    "    plt.show()\n",
    "\n",
    "    # Find the elbow point\n",
    "    diff = np.diff(bic_scores)\n",
    "    elbow_index = np.argmax(diff) + 1\n",
    "    optimal_n_components = n_components_range[elbow_index]\n",
    "    print(f'The optimal number of components is {optimal_n_components}')\n",
    "\n",
    "    return optimal_n_components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposition info:\n",
      "  normalize_poissonian_noise=True\n",
      "  algorithm=Pipeline(steps=[('pca', PCA(n_components=40, svd_solver='full'))])\n",
      "  output_dimension=40\n",
      "  centre=None\n",
      "scikit-learn estimator:\n",
      "Pipeline(steps=[('pca', PCA(n_components=40, svd_solver='full'))])\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([(\"pca\", PCA(n_components=40, svd_solver='full'))])\n",
    "xrf_decomp = xrf_stack.decomposition(normalize_poissonian_noise = True, algorithm = pipeline, \n",
    "                        return_info = True, output_dimension = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: title={'center': 'data\\nPCA Scree Plot'}, xlabel='Principal component index', ylabel='Proportion of variance'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xrf_stack.plot_cumulative_explained_variance_ratio()\n",
    "xrf_stack.plot_explained_variance_ratio(log=True, vline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_components = xrf_stack.estimate_elbow_position()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_loadings = xrf_stack.get_decomposition_loadings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_loadings_array = pca_loadings.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_loadings = pca_loadings_array[:, :sig_components]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1.37685980e-01  1.72576629e-01  1.72863596e-01 ...  5.89302388e-01\n",
      "    5.86153879e-01  6.06618868e-01]\n",
      "  [ 1.88884626e-01  1.87804333e-01  1.77575387e-01 ...  5.93637986e-01\n",
      "    6.08787786e-01  6.19968544e-01]\n",
      "  [ 1.65428140e-01  1.86507331e-01  1.90446821e-01 ...  5.85444089e-01\n",
      "    5.89406450e-01  5.95153159e-01]\n",
      "  [ 1.81175804e-01  1.84414361e-01  1.95640126e-01 ...  5.88841104e-01\n",
      "    5.92084979e-01  6.10808209e-01]\n",
      "  [ 1.88537654e-01  1.90289721e-01  2.01769547e-01 ...  5.85773315e-01\n",
      "    5.83278310e-01  5.91334475e-01]\n",
      "  [ 2.01359227e-01  2.16007329e-01  2.21325518e-01 ...  5.79508783e-01\n",
      "    6.03322480e-01  6.20481009e-01]]\n",
      "\n",
      " [[ 4.30799528e-01  4.43868302e-01  4.59377346e-01 ... -1.10034374e-01\n",
      "   -1.13082691e-01 -1.03179541e-01]\n",
      "  [ 4.54317662e-01  4.54889093e-01  4.54915223e-01 ... -1.09784272e-01\n",
      "   -1.17582605e-01 -1.05575097e-01]\n",
      "  [ 4.61490467e-01  4.66712110e-01  4.61819571e-01 ... -1.09128430e-01\n",
      "   -1.25784143e-01 -1.39770530e-01]\n",
      "  [ 4.69371158e-01  4.63185534e-01  4.65510184e-01 ... -1.03157655e-01\n",
      "   -1.31219532e-01 -1.29611836e-01]\n",
      "  [ 4.57076488e-01  4.72718939e-01  4.73301100e-01 ... -1.05885229e-01\n",
      "   -1.12445807e-01 -1.38026704e-01]\n",
      "  [ 4.64529089e-01  4.85981867e-01  4.91929017e-01 ... -1.19535309e-01\n",
      "   -1.36740685e-01 -1.06080605e-01]]\n",
      "\n",
      " [[-3.43005429e-03 -5.37788086e-03  1.10195184e-02 ... -4.51230315e-02\n",
      "   -4.32017398e-02 -4.81126988e-02]\n",
      "  [-2.62141206e-03 -6.48572530e-03 -2.07815006e-03 ... -4.68020058e-02\n",
      "   -5.08227323e-02 -4.54329279e-02]\n",
      "  [-1.23987342e-04  7.88370569e-04  4.88359240e-03 ... -4.51600887e-02\n",
      "   -4.51261274e-02 -5.03626753e-02]\n",
      "  [-4.30371914e-03  3.01709765e-03 -9.06987087e-03 ... -4.36588779e-02\n",
      "   -5.12897287e-02 -5.13671537e-02]\n",
      "  [-6.33418922e-03  3.93608999e-03 -6.46944087e-03 ... -4.61500337e-02\n",
      "   -4.64003767e-02 -4.71772927e-02]\n",
      "  [-9.09180463e-04  1.99470123e-04  3.87759042e-03 ... -4.53952553e-02\n",
      "   -5.11908676e-02 -4.76938529e-02]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 2.34914825e-02 -1.14227824e-02  1.28418284e-02 ...  4.88515354e-03\n",
      "   -1.73802466e-03 -4.79216439e-03]\n",
      "  [ 2.22461174e-02  5.01376556e-03 -4.48975996e-03 ... -7.69218038e-03\n",
      "    2.15064477e-03 -1.19030160e-02]\n",
      "  [-7.29028289e-04 -7.46056731e-03  7.58663628e-03 ... -7.16158834e-04\n",
      "   -9.35160575e-03  1.10998662e-02]\n",
      "  [-1.07800664e-02 -8.51761945e-03  8.72995393e-04 ... -5.85082742e-03\n",
      "    1.26389787e-03  4.07136565e-03]\n",
      "  [ 2.44901858e-02 -2.05025306e-03  1.56236483e-02 ...  3.36343913e-03\n",
      "   -5.64701095e-03 -3.63488179e-03]\n",
      "  [ 7.96729664e-03 -1.03176370e-04  2.81863247e-03 ... -2.34953567e-03\n",
      "   -3.03471976e-03  1.63644169e-02]]\n",
      "\n",
      " [[-1.41667074e-02 -2.30036705e-03 -8.30991040e-03 ...  2.57534618e-03\n",
      "    2.13620200e-03  2.39317926e-03]\n",
      "  [-4.96701621e-03 -9.25219907e-03 -2.80668715e-03 ... -6.77699356e-03\n",
      "    6.22660458e-03  9.10518723e-03]\n",
      "  [-1.44962248e-02  1.12925721e-02  2.83872051e-03 ...  9.83692406e-03\n",
      "   -1.58584426e-03  2.01539989e-02]\n",
      "  [-4.41573242e-03  2.21116663e-03 -7.59659040e-03 ... -9.46764254e-04\n",
      "   -6.13601796e-03  1.39040487e-03]\n",
      "  [-9.21168852e-03 -2.16166103e-02 -3.89869812e-03 ... -1.02773320e-03\n",
      "   -6.69035915e-03 -7.37576561e-03]\n",
      "  [-5.48735496e-03 -7.76907989e-03 -6.02051203e-04 ... -8.05023726e-03\n",
      "    7.53614590e-03 -1.18651551e-02]]\n",
      "\n",
      " [[-1.89084175e-02 -6.50439242e-03  1.64049647e-02 ...  1.21896955e-03\n",
      "   -5.09128181e-03 -4.30685963e-03]\n",
      "  [ 2.41662200e-03  6.98010472e-03  1.86788405e-02 ...  1.07330530e-02\n",
      "   -5.32284918e-03  6.99502227e-04]\n",
      "  [-2.71817612e-03  4.58305859e-05  9.20485713e-03 ... -1.03040996e-02\n",
      "    1.26311976e-02  8.73983052e-03]\n",
      "  [-9.83088419e-04  6.10678496e-03 -6.85022993e-03 ... -2.82251769e-03\n",
      "   -2.14169258e-03  1.12615690e-02]\n",
      "  [ 5.87924696e-03  1.62438953e-02  9.43242786e-03 ...  5.73919716e-04\n",
      "    5.80309525e-06  5.37916145e-03]\n",
      "  [-3.77483385e-03 -4.46908679e-03 -1.51445152e-02 ...  7.77257845e-03\n",
      "    7.73167929e-03 -3.09730517e-03]]]\n"
     ]
    }
   ],
   "source": [
    "print(sig_loadings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the number of rows and columns for the subplot grid\n",
    "num_pcs = sig_components - 1\n",
    "num_cols = 3\n",
    "num_rows = int(np.ceil(num_pcs / num_cols))\n",
    "\n",
    "# Create a figure and a grid of subplots\n",
    "fig, axs = plt.subplots(num_rows, num_cols, figsize=(15, 5*num_rows))\n",
    "\n",
    "# Iterate over the subplots and plot the data\n",
    "for i in range(num_pcs):\n",
    "    row = i // num_cols\n",
    "    col = i % num_cols\n",
    "    axs[row, col].scatter(sig_loadings[:, 0], sig_loadings[:, i+1])\n",
    "    axs[row, col].set_xlabel(\"PC 1\")\n",
    "    axs[row, col].set_ylabel(\"PC {}\".format(i+2))\n",
    "\n",
    "# Remove any extra subplots that don't have any data\n",
    "for i in range(num_pcs, num_rows*num_cols):\n",
    "    axs.flat[i].remove()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "should have a nice array of PC plots\n",
    "  - it is not saved though..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposition info:\n",
      "  normalize_poissonian_noise=True\n",
      "  algorithm=Pipeline(steps=[('fa', FactorAnalysis(n_components=5))])\n",
      "  output_dimension=5\n",
      "  centre=None\n",
      "scikit-learn estimator:\n",
      "Pipeline(steps=[('fa', FactorAnalysis(n_components=5))])\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([(\"fa\", FactorAnalysis(n_components=5))])\n",
    "xrf_decomp = xrf_stack.decomposition(normalize_poissonian_noise=True, \n",
    "                                      algorithm=pipeline, \n",
    "                                      return_info=True, \n",
    "                                      output_dimension=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b7ccd8c37ac4a22abcd00b0bfe0c0bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Label(value='Decomposition component index', layout=Layout(width='15%')), IntSli…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xrf_stack.plot_decomposition_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa_load = xrf_stack.get_decomposition_loadings()\n",
    "#fa_load.save('snv_eds_analysis/plage_TD_eds_FA_4_load')\n",
    "\n",
    "fa_fact = xrf_stack.get_decomposition_factors()\n",
    "#fa_fact.set_elements(elements)\n",
    "\n",
    "\n",
    "#fa_fact.save('snv_eds_analysis/plage_TD_eds_FA_4_fact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa_load_array = fa_load.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the number of rows and columns for the subplot grid\n",
    "num_pcs = sig_components - 1\n",
    "num_cols = 3\n",
    "num_rows = int(np.ceil(num_pcs / num_cols))\n",
    "\n",
    "# Create a figure and a grid of subplots\n",
    "fig, axs = plt.subplots(num_rows, num_cols, figsize=(15, 5*num_rows))\n",
    "\n",
    "# Iterate over the subplots and plot the data\n",
    "for i in range(num_pcs):\n",
    "    row = i // num_cols\n",
    "    col = i % num_cols\n",
    "    axs[row, col].scatter(sig_loadings[:, 0], sig_loadings[:, i+1])\n",
    "    axs[row, col].set_xlabel(\"PC 1\")\n",
    "    axs[row, col].set_ylabel(\"PC {}\".format(i+2))\n",
    "\n",
    "# Remove any extra subplots that don't have any data\n",
    "for i in range(num_pcs, num_rows*num_cols):\n",
    "    axs.flat[i].remove()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa_facts, ydim,xdim = fa_load.data.shape\n",
    "\n",
    "fact_load_vect = pd.DataFrame((fa_load.data.reshape(fa_facts, ydim * xdim).T), columns = ['Factor 1','Factor 2','Factor 3','Factor 4'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa_facts, ydim,xdim = fa_load.data.shape\n",
    "fa_vect = fa_load.data.reshape(fa_facts,ydim*xdim).transpose()\n",
    "fa_vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ICA on PCA loadings\n",
    "blind source separation (BSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings_to_cluster = fa_vect[:,:fa_facts]\n",
    "loadings_to_cluster.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clus=bic\n",
    "pgk = PGK(n_clusters =num_clus, n_init=10).fit(loadings_to_cluster)\n",
    "# Process results for visualisation\n",
    "print(pgk.memberships_)\n",
    "labels_ = np.argmax(pgk.memberships_, axis=1)\n",
    "memberships_ = pgk.memberships_[range(len(pgk.memberships_)), labels_] \n",
    "\n",
    "labels = labels_.reshape([ydim,xdim])\n",
    "labels=hs.signals.Signal2D(labels)\n",
    "labels.plot(cmap='tab10')\n",
    "\n",
    "mems=pgk.memberships_.reshape(ydim,xdim,num_clus)\n",
    "print(mems.shape)\n",
    "\n",
    "mem_maps=hs.signals.Signal2D(mems)\n",
    "mem_maps=mem_maps.transpose(signal_axes=(2,0))\n",
    "mem_maps.change_dtype('float32')\n",
    "mem_maps.plot(cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clus=bic\n",
    "pgk = PGK(n_clusters =num_clus, n_init=10).fit(loadings_to_cluster)\n",
    "# Process results for visualisation\n",
    "print(pgk.memberships_)\n",
    "labels_ = np.argmax(pgk.memberships_, axis=1)\n",
    "memberships_ = pgk.memberships_[range(len(pgk.memberships_)), labels_] \n",
    "\n",
    "labels = labels_.reshape([ydim,xdim])\n",
    "labels=hs.signals.Signal2D(labels)\n",
    "labels.plot(cmap='tab10')\n",
    "\n",
    "mems=pgk.memberships_.reshape(ydim,xdim,num_clus)\n",
    "print(mems.shape)\n",
    "\n",
    "mem_maps=hs.signals.Signal2D(mems)\n",
    "mem_maps=mem_maps.transpose(signal_axes=(2,0))\n",
    "mem_maps.change_dtype('float32')\n",
    "mem_maps.plot(cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal sums from regions of interest\n",
    "selected from the dimension reduction and the clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# working..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensions of the map\n",
    "also sets save paths..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrf_map.change_dtype('float32')\n",
    "#xrf_map.save(save_path+'map1')\n",
    "\n",
    "xrf_stack = xrf_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Signal1D, title: , dimensions: (201, 201|4096)>"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xrf_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrf_stack.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwrite '/Users/joshuashea/melt_mapsat16_map2_002_mapped_crop.hspy' (y/n)?\n",
      "y\n"
     ]
    }
   ],
   "source": [
    "xrf_stack.change_dtype('float32')\n",
    "\n",
    "\n",
    "xrf_stack.save(save_path+'at16_map2_002_mapped_crop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposition info:\n",
      "  normalize_poissonian_noise=False\n",
      "  algorithm=sklearn_pca\n",
      "  output_dimension=20\n",
      "  centre=None\n",
      "scikit-learn estimator:\n",
      "PCA(n_components=20)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04bbfc07b5f943f3b9530220f44fa788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Label(value='Decomposition component index', layout=Layout(width='15%')), IntSli…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "xrf_stack.decomposition(normalize_poissonian_noise=False, algorithm=\"sklearn_pca\", output_dimension=20)\n",
    "\n",
    "xrf_stack.plot_explained_variance_ratio(log=True, vline=True)\n",
    "\n",
    "xrf_stack.plot_decomposition_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Principal component', ylabel='Cumulative explained variance ratio'>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xrf_stack.plot_cumulative_explained_variance_ratio()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**note** iterative cropping above showed that most data/ analysis was better when cropping down to the shape (157, 63|2001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrf_stack.save(save_path+'lisheen_low_res_map_data_Crop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrf_stack = xrf_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrf_stack.decomposition(normalize_poissonian_noise=True, algorithm='PCA', output_dimension=15)\n",
    "xrf_stack.plot_cumulative_explained_variance_ratio()\n",
    "xrf_stack.plot_explained_variance_ratio(log=True, vline=True)\n",
    "xrf_stack.plot_decomposition_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bic_elbow(signal, decomposition_method='sklearn_pca', n_components_range=np.arange(2, 11)):\n",
    "    \"\"\"\n",
    "    Perform a BIC elbow test to find the optimal number of components for a signal decomposition method.\n",
    "    Parameters\n",
    "    ----------\n",
    "    signal : `hyperspy.signals.Signal2D` or `hyperspy.signals.SignalND`\n",
    "        The hyperspy signal to perform the BIC elbow test on.\n",
    "    decomposition_method : str, optional\n",
    "        The signal decomposition method to use. Default is 'PCA'.\n",
    "    n_components_range : array-like, optional\n",
    "        The range of possible numbers of components to fit the signal model for.\n",
    "        Default is np.arange(2, 11).\n",
    "    Returns\n",
    "    -------\n",
    "    optimal_n_components : int\n",
    "        The optimal number of components that minimizes the BIC score.\n",
    "    Notes\n",
    "    -----\n",
    "    This function uses the Bayesian Information Criterion (BIC) to select the optimal number of components\n",
    "    for the given signal decomposition method. The BIC score is computed for each number of components\n",
    "    in the range, and the elbow point in the BIC curve is identified as the optimal number of components.\n",
    "    \"\"\"\n",
    "    bic_scores = []\n",
    "    for n_components in n_components_range:\n",
    "        # Fit the signal model\n",
    "        if decomposition_method == 'sklearn_pca':\n",
    "            model = signal.decomposition.PCA(n_components=n_components)\n",
    "        elif decomposition_method == 'NMF':\n",
    "            model = signal.decomposition.nmf(n_components=n_components)\n",
    "        elif decomposition_method == 'ICA':\n",
    "            model = signal.decomposition.ica(n_components=n_components)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown decomposition method: {decomposition_method}\")\n",
    "\n",
    "        # Compute the log-likelihood and number of model parameters\n",
    "        log_likelihood = model.score(signal)\n",
    "        n_samples, n_features = signal.data.shape\n",
    "        n_params = n_components * (n_features + 1)\n",
    "\n",
    "        # Compute the BIC score\n",
    "        bic = -2 * log_likelihood + n_params * np.log(n_samples)\n",
    "        bic_scores.append(bic)\n",
    "\n",
    "    # Plot the BIC scores as a function of the number of components\n",
    "    plt.plot(n_components_range, bic_scores, 'o-')\n",
    "    plt.xlabel('Number of components')\n",
    "    plt.ylabel('BIC score')\n",
    "    plt.title(f'BIC elbow test for {decomposition_method} decomposition')\n",
    "    plt.show()\n",
    "\n",
    "    # Find the elbow point\n",
    "    diff = np.diff(bic_scores)\n",
    "    elbow_index = np.argmax(diff) + 1\n",
    "    optimal_n_components = n_components_range[elbow_index]\n",
    "    print(f'The optimal number of components is {optimal_n_components}')\n",
    "\n",
    "    return optimal_n_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'PCA'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbic_elbow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxrf_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecomposition_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msklearn_pca\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_components_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[32], line 27\u001b[0m, in \u001b[0;36mbic_elbow\u001b[0;34m(signal, decomposition_method, n_components_range)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n_components \u001b[38;5;129;01min\u001b[39;00m n_components_range:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# Fit the signal model\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m decomposition_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msklearn_pca\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 27\u001b[0m         model \u001b[38;5;241m=\u001b[39m \u001b[43msignal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecomposition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPCA\u001b[49m(n_components\u001b[38;5;241m=\u001b[39mn_components)\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m decomposition_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNMF\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     29\u001b[0m         model \u001b[38;5;241m=\u001b[39m signal\u001b[38;5;241m.\u001b[39mdecomposition\u001b[38;5;241m.\u001b[39mnmf(n_components\u001b[38;5;241m=\u001b[39mn_components)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'PCA'"
     ]
    }
   ],
   "source": [
    "bic_elbow(xrf_map, decomposition_method='sklearn_pca', n_components_range=np.arange(2, 11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown decomposition method: PCA",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m     xrf_data \u001b[38;5;241m=\u001b[39m h5[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/xrmmap/mcasum/counts\u001b[39m\u001b[38;5;124m'\u001b[39m][:]\n\u001b[1;32m      4\u001b[0m xrf_map \u001b[38;5;241m=\u001b[39m hs\u001b[38;5;241m.\u001b[39msignals\u001b[38;5;241m.\u001b[39mSignal1D(xrf_data)\n\u001b[0;32m----> 6\u001b[0m optimal_n_components \u001b[38;5;241m=\u001b[39m \u001b[43mbic_elbow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxrf_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecomposition_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPCA\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_components_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 33\u001b[0m, in \u001b[0;36mbic_elbow\u001b[0;34m(signal, decomposition_method, n_components_range)\u001b[0m\n\u001b[1;32m     31\u001b[0m     model \u001b[38;5;241m=\u001b[39m signal\u001b[38;5;241m.\u001b[39mdecomposition\u001b[38;5;241m.\u001b[39mica(n_components\u001b[38;5;241m=\u001b[39mn_components)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown decomposition method: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdecomposition_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Compute the log-likelihood and number of model parameters\u001b[39;00m\n\u001b[1;32m     36\u001b[0m log_likelihood \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mscore(signal)\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown decomposition method: PCA"
     ]
    }
   ],
   "source": [
    "with h5py.File(data, 'r') as h5:\n",
    "    xrf_data = h5['/xrmmap/mcasum/counts'][:]\n",
    "\n",
    "xrf_map = hs.signals.Signal1D(xrf_data)\n",
    "\n",
    "optimal_n_components = bic_elbow(xrf_map, decomposition_method='PCA', n_components_range=np.arange(2, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## re-explore\n",
    "\n",
    "get 6 good factors... need to label peaks etc but starting to pull out data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joshuashea/xraylarch/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:289: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposition info:\n",
      "  normalize_poissonian_noise=False\n",
      "  algorithm=NMF\n",
      "  output_dimension=5\n",
      "  centre=None\n",
      "scikit-learn estimator:\n",
      "NMF(n_components=5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joshuashea/xraylarch/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1637: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcdb581e536645a5b27c857af180b408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Label(value='Decomposition component index', layout=Layout(width='15%')), IntSli…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xrf_stack.decomposition(normalize_poissonian_noise=False, algorithm='NMF', output_dimension=5)\n",
    "\n",
    "\n",
    "xrf_stack.plot_decomposition_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrf_stack.decomposition(normalize_poissonian_noise=True, algorithm='NMF', output_dimension=5)\n",
    "\n",
    "\n",
    "xrf_stack.plot_decomposition_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## examine with factor analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposition info:\n",
      "  normalize_poissonian_noise=False\n",
      "  algorithm=Pipeline(steps=[('FA', FactorAnalysis(n_components=5, rotation='varimax'))])\n",
      "  output_dimension=11\n",
      "  centre=None\n",
      "scikit-learn estimator:\n",
      "Pipeline(steps=[('FA', FactorAnalysis(n_components=5, rotation='varimax'))])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4941caacdc514b1db784b6f60801988a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Label(value='Decomposition component index', layout=Layout(width='15%')), IntSli…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline = Pipeline([(\"FA\", FactorAnalysis(n_components=5,rotation = 'varimax'))])\n",
    "\n",
    "xrf_stack.decomposition(normalize_poissonian_noise=False, algorithm=pipeline, return_info=True,output_dimension=11)\n",
    "\n",
    "xrf_stack.plot_decomposition_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Label(value='Decomposition component index', layout=Layout(width='15%')), IntSli…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xrf_stack.plot_decomposition_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa_load= xrf_stack.get_decomposition_loadings()\n",
    "#fa_load.save('snv_eds_analysis/plage_TD_eds_FA_4_load')\n",
    "\n",
    "fa_fact= xrf_stack.get_decomposition_factors()\n",
    "#fa_fact.set_elements(elements)\n",
    "\n",
    "\n",
    "#fa_fact.save('snv_eds_analysis/plage_TD_eds_FA_4_fact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa_facts, ydim,xdim=fa_load.data.shape\n",
    "\n",
    "fact_load_vect= pd.DataFrame((fa_load.data.reshape(fa_facts, ydim*xdim).T), columns = ['Factor 1','Factor 2','Factor 3','Factor 4'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fact_load_vect' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [126], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfact_load_vect\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fact_load_vect' is not defined"
     ]
    }
   ],
   "source": [
    "fact_load_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa_facts, ydim,xdim=fa_load.data.shape\n",
    "fa_vect=fa_load.data.reshape(fa_facts,ydim*xdim).transpose()\n",
    "fa_vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings_to_cluster=fa_vect[:,:fa_facts]\n",
    "loadings_to_cluster.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Probabilistic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mPGK\u001b[39;00m(\u001b[43mProbabilistic\u001b[49m, GustafsonKesselMixin):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Probabilistic' is not defined"
     ]
    }
   ],
   "source": [
    "class PGK(Probabilistic, GustafsonKesselMixin):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clus=bic\n",
    "pgk = PGK(n_clusters =num_clus, n_init=10).fit(loadings_to_cluster)\n",
    "# Process results for visualisation\n",
    "print(pgk.memberships_)\n",
    "labels_ = np.argmax(pgk.memberships_, axis=1)\n",
    "memberships_ = pgk.memberships_[range(len(pgk.memberships_)), labels_] \n",
    "\n",
    "labels = labels_.reshape([ydim,xdim])\n",
    "labels=hs.signals.Signal2D(labels)\n",
    "labels.plot(cmap='tab10')\n",
    "\n",
    "mems=pgk.memberships_.reshape(ydim,xdim,num_clus)\n",
    "print(mems.shape)\n",
    "\n",
    "mem_maps=hs.signals.Signal2D(mems)\n",
    "mem_maps=mem_maps.transpose(signal_axes=(2,0))\n",
    "mem_maps.change_dtype('float32')\n",
    "mem_maps.plot(cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "\n",
    "mem_maps.plot(cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## next steps\n",
    "\n",
    "1. makes sense to calcualte an elbow test or examine the BIC to determine the 'optimal' number of clusters\n",
    "2. explore applying HDBSCAN on the FA/ PCA factors\n",
    "3. would also to explore the UMAP pretreatment then HDBSCAN\n",
    "4. maybe blind source separation (ie ICA) after PCA.\n",
    "\n",
    "for 2 and 3 need a different virtual enviroment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
