{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling XRF HDF5 datatype, extracting and handling multidimensional XRF data\n",
    "import h5py \n",
    "import hyperspy.api as hs\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib qt5\n",
    "\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import scipy\n",
    "\n",
    "\n",
    "#Dimension reduction and clustering\n",
    "import umap\n",
    "import hdbscan\n",
    "import sklearn as skl\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "import sklearn.cluster as cluster\n",
    "# from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert XRF data to Hyperspy standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mISE_500sqaures_A21-016_Map1_001.h5\u001b[m\u001b[m\n",
      "\u001b[31mISE_500sqaures_A21_054_botom_right_map_center_001.h5\u001b[m\u001b[m\n",
      "map.hspy\n",
      "map1.hspy\n",
      "map<class 'map'>.hspy\n",
      "/Users/user/Documents/github/melt_maps\n"
     ]
    }
   ],
   "source": [
    "!cd '/Users/user/Documents/Projects/XRF_machine_learning/data'\n",
    "!ls '/Users/user/Documents/Projects/XRF_machine_learning/data'\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = '/Users/user/Documents/Projects/XRF_machine_learning/data/ISE_500sqaures_A21_054_botom_right_map_center_001.h5'\n",
    "save_path = '/Users/user/Documents/Projects/XRF_machine_learning/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_and_list_h5_objects(obj, level=0, path='', dataset_list=[]):\n",
    "    \"\"\"\n",
    "   checks whether the input object is a file or group, prints its name/type \n",
    "   accordingly, and then goes through its subgroups and datasets, printing \n",
    "   their names/types and adding the dataset paths to the list 'dataset_list'.\n",
    "   \n",
    "   Returns\n",
    "    -------\n",
    "    prints the structure of the HDF5 file and returns a list of dataset paths\n",
    "    \n",
    "   Note\n",
    "    -----\n",
    "    This function makes it easier to look through the HDF5 blackbox and call \n",
    "    datasets.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, h5py.File):\n",
    "        print(\"File name: {}\".format(obj.filename))\n",
    "        obj = obj['/']\n",
    "    elif isinstance(obj, h5py.Group):\n",
    "        print(\"Group: {}\".format(obj.name))\n",
    "\n",
    "    for key, val in obj.items():\n",
    "        new_path = path + '/' + key if path else key\n",
    "        if isinstance(val, h5py.Group):\n",
    "            print(\"  \" * level + \"Group: {}\".format(key))\n",
    "            print_and_list_h5_objects(val, level + 1, new_path, dataset_list)\n",
    "        else:\n",
    "            print(\"  \" * level + \"Dataset: {}\".format(key))\n",
    "            dataset_list.append(new_path)\n",
    "    \n",
    "    return dataset_list\n",
    "\n",
    "# USE EXAMPLE\n",
    "# with h5py.File(data,'r') as h5:\n",
    "#    datasets = print_and_list_h5_objects(h5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File name: /Users/user/Documents/Projects/XRF_machine_learning/data/ISE_500sqaures_A21_054_botom_right_map_center_001.h5\n",
      "Group: xrmmap\n",
      "Group: /xrmmap\n",
      "  Group: areas\n",
      "Group: /xrmmap/areas\n",
      "    Dataset: A21-054_Br_Xanes_spot\n",
      "    Dataset: A21-054_Br_Xanes_spot_2\n",
      "    Dataset: A21-054_I_Xanes_spot\n",
      "    Dataset: A21-054_I_Xanes_spot_2\n",
      "    Dataset: area_003\n",
      "    Dataset: area_004\n",
      "  Group: config\n",
      "Group: /xrmmap/config\n",
      "    Group: environ\n",
      "Group: /xrmmap/config/environ\n",
      "      Dataset: address\n",
      "      Dataset: name\n",
      "      Dataset: value\n",
      "    Group: general\n",
      "Group: /xrmmap/config/general\n",
      "      Dataset: basedir\n",
      "      Dataset: envfile\n",
      "    Group: mca_calib\n",
      "Group: /xrmmap/config/mca_calib\n",
      "      Dataset: offset\n",
      "      Dataset: quad\n",
      "      Dataset: slope\n",
      "    Group: mca_settings\n",
      "Group: /xrmmap/config/mca_settings\n",
      "    Group: motor_controller\n",
      "Group: /xrmmap/config/motor_controller\n",
      "      Dataset: group\n",
      "      Dataset: host\n",
      "      Dataset: mode\n",
      "      Dataset: passwd\n",
      "      Dataset: positioners\n",
      "      Dataset: type\n",
      "      Dataset: user\n",
      "    Group: notes\n",
      "Group: /xrmmap/config/notes\n",
      "    Group: positioners\n",
      "Group: /xrmmap/config/positioners\n",
      "      Dataset: 13IDE:En:Energy\n",
      "      Dataset: 13IDE:m19\n",
      "      Dataset: 13IDE:m25\n",
      "      Dataset: 13IDE:m28\n",
      "      Dataset: 13IDE:m31\n",
      "      Dataset: 13IDE:m32\n",
      "      Dataset: 13IDE:m34\n",
      "      Dataset: 13IDE:m35\n",
      "      Dataset: 13IDE:m36\n",
      "      Dataset: 13IDE:m39\n",
      "      Dataset: 13IDE:m4\n",
      "      Dataset: 13IDE:m40\n",
      "      Dataset: 13IDE:userTran5.A\n",
      "      Dataset: 13XRM:ANA:Energy\n",
      "      Dataset: 13XRM:m1\n",
      "      Dataset: 13XRM:m10\n",
      "      Dataset: 13XRM:m11\n",
      "      Dataset: 13XRM:m2\n",
      "      Dataset: 13XRM:m3\n",
      "      Dataset: 13XRM:m4\n",
      "      Dataset: 13XRM:m5\n",
      "      Dataset: 13XRM:m6\n",
      "    Group: rois\n",
      "Group: /xrmmap/config/rois\n",
      "      Dataset: address\n",
      "      Dataset: limits\n",
      "      Dataset: name\n",
      "    Group: scan\n",
      "Group: /xrmmap/config/scan\n",
      "      Dataset: comments\n",
      "      Dataset: dimension\n",
      "      Dataset: filename\n",
      "      Dataset: pos1\n",
      "      Dataset: pos2\n",
      "      Dataset: start1\n",
      "      Dataset: start2\n",
      "      Dataset: step1\n",
      "      Dataset: step2\n",
      "      Dataset: stop1\n",
      "      Dataset: stop2\n",
      "      Dataset: text\n",
      "      Dataset: time1\n",
      "  Group: mcasum\n",
      "Group: /xrmmap/mcasum\n",
      "    Dataset: counts\n",
      "    Dataset: dtfactor\n",
      "    Dataset: energy\n",
      "    Dataset: inpcounts\n",
      "    Dataset: livetime\n",
      "    Dataset: outcounts\n",
      "    Dataset: realtime\n",
      "  Group: positions\n",
      "Group: /xrmmap/positions\n",
      "    Dataset: address\n",
      "    Dataset: name\n",
      "    Dataset: pos\n",
      "  Group: roimap\n",
      "Group: /xrmmap/roimap\n",
      "    Dataset: det_address\n",
      "    Dataset: det_cor\n",
      "    Dataset: det_name\n",
      "    Dataset: det_raw\n",
      "    Group: mcasum\n",
      "Group: /xrmmap/roimap/mcasum\n",
      "      Group: As Ka\n",
      "Group: /xrmmap/roimap/mcasum/As Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Ba La\n",
      "Group: /xrmmap/roimap/mcasum/Ba La\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Br Ka\n",
      "Group: /xrmmap/roimap/mcasum/Br Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Ca Ka\n",
      "Group: /xrmmap/roimap/mcasum/Ca Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Ce La\n",
      "Group: /xrmmap/roimap/mcasum/Ce La\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Cl Ka\n",
      "Group: /xrmmap/roimap/mcasum/Cl Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Co Ka\n",
      "Group: /xrmmap/roimap/mcasum/Co Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Cr Ka\n",
      "Group: /xrmmap/roimap/mcasum/Cr Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Cu Ka\n",
      "Group: /xrmmap/roimap/mcasum/Cu Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Eu La\n",
      "Group: /xrmmap/roimap/mcasum/Eu La\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Fe Ka\n",
      "Group: /xrmmap/roimap/mcasum/Fe Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Ge Ka\n",
      "Group: /xrmmap/roimap/mcasum/Ge Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: I La1\n",
      "Group: /xrmmap/roimap/mcasum/I La1\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: I Lb1\n",
      "Group: /xrmmap/roimap/mcasum/I Lb1\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: K Ka\n",
      "Group: /xrmmap/roimap/mcasum/K Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Mn Ka\n",
      "Group: /xrmmap/roimap/mcasum/Mn Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Mo Ka\n",
      "Group: /xrmmap/roimap/mcasum/Mo Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Ni Ka\n",
      "Group: /xrmmap/roimap/mcasum/Ni Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: OutputCounts\n",
      "Group: /xrmmap/roimap/mcasum/OutputCounts\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: P Ka\n",
      "Group: /xrmmap/roimap/mcasum/P Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Pb La1\n",
      "Group: /xrmmap/roimap/mcasum/Pb La1\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Rb Ka\n",
      "Group: /xrmmap/roimap/mcasum/Rb Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Re La1\n",
      "Group: /xrmmap/roimap/mcasum/Re La1\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: S Ka\n",
      "Group: /xrmmap/roimap/mcasum/S Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Sc Ka\n",
      "Group: /xrmmap/roimap/mcasum/Sc Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Se Ka\n",
      "Group: /xrmmap/roimap/mcasum/Se Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Si Ka\n",
      "Group: /xrmmap/roimap/mcasum/Si Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Sr Ka\n",
      "Group: /xrmmap/roimap/mcasum/Sr Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Ti Ka\n",
      "Group: /xrmmap/roimap/mcasum/Ti Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: V Ka\n",
      "Group: /xrmmap/roimap/mcasum/V Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: V Kb\n",
      "Group: /xrmmap/roimap/mcasum/V Kb\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Y Ka\n",
      "Group: /xrmmap/roimap/mcasum/Y Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Zn Ka\n",
      "Group: /xrmmap/roimap/mcasum/Zn Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Zr Ka\n",
      "Group: /xrmmap/roimap/mcasum/Zr Ka\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "      Group: Zr L\n",
      "Group: /xrmmap/roimap/mcasum/Zr L\n",
      "        Dataset: cor\n",
      "        Dataset: limits\n",
      "        Dataset: raw\n",
      "    Dataset: sum_cor\n",
      "    Dataset: sum_list\n",
      "    Dataset: sum_name\n",
      "    Dataset: sum_raw\n",
      "  Group: scalars\n",
      "Group: /xrmmap/scalars\n",
      "    Dataset: I0\n",
      "    Dataset: I0_raw\n",
      "    Dataset: I1\n",
      "    Dataset: I1_raw\n",
      "    Dataset: I2\n",
      "    Dataset: I2_raw\n",
      "    Dataset: TSCALER\n",
      "    Dataset: TSCALER_raw\n",
      "  Group: work\n",
      "Group: /xrmmap/work\n",
      "  Group: xrd1d\n",
      "Group: /xrmmap/xrd1d\n"
     ]
    }
   ],
   "source": [
    "# prints the h5 file structure\n",
    "with h5py.File(data,'r') as h5:\n",
    "    datasets = print_and_list_h5_objects(h5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracts congiuration settings of the run\n",
    "needed to correctly scale axes and the scale channels to KeV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     0         0\n",
      "0                 Experiment.User_Name      Ezad\n",
      "1           Experiment.Proposal_Number     78722\n",
      "2        Experiment.Beam_Size__Nominal       2um\n",
      "3     Experiment.Monochromator_Crystal   Si(111)\n",
      "4   Experiment.Double_H_Mirror_Stripes   rhodium\n",
      "..                                 ...       ...\n",
      "60                 SampleStage.CoarseY  177.9370\n",
      "61                   SampleStage.FineX  -0.25500\n",
      "62                   SampleStage.FineY         0\n",
      "63                   SampleStage.Theta   90.0000\n",
      "64                    Undulator.Energy    13.717\n",
      "\n",
      "[65 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# extracts the congiuration settings of the scan and beamline\n",
    "with h5py.File(data, 'r') as h5:\n",
    "    dataset_df = pd.DataFrame(h5['/xrmmap/config/environ/name'][:])\n",
    "    value_df = pd.DataFrame(h5['/xrmmap/config/environ/value'][:])\n",
    "    # Merges configuration name and value dataframes\n",
    "    values_df = pd.concat([dataset_df, value_df], axis=1)\n",
    "    # Decodes byte strings to regular strings\n",
    "    values_df = values_df.apply(lambda x: x.str.decode('utf-8') if isinstance(x[0], bytes) else x, axis=0)\n",
    "print(values_df)\n",
    "# Save dataframe to CSV file\n",
    "#values_df.to_csv('values_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Axis management and scaling\n",
    "\n",
    "these are needed to scale the data and pixelscorrectly etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(201, 201, 4096)\n"
     ]
    }
   ],
   "source": [
    "# extracts xrf map and channels from the HDF5 file\n",
    "with h5py.File(data, 'r') as h5:\n",
    "    xrf_data = h5['/xrmmap/mcasum/counts'][:]\n",
    "    shape = h5['/xrmmap/mcasum/counts'].shape\n",
    "print(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts xrf map to hyperspy and scales data\n",
    "\n",
    "def convert_xrf_to_hyperspy(xrf_data):\n",
    "    # converts xrf map to hyperspy\n",
    "    xrf_map = hs.signals.Signal1D(xrf_data)\n",
    "\n",
    "    # scales the x, y and theta of the hyperspy data\n",
    "    xrf_map.axes_manager.navigation_axes[0].name = 'x'\n",
    "    xrf_map.axes_manager.navigation_axes[0].units = 'mm'\n",
    "    xrf_map.axes_manager.navigation_axes[0].scale = 0.025\n",
    "\n",
    "    xrf_map.axes_manager.navigation_axes[1].name = 'y'\n",
    "    xrf_map.axes_manager.navigation_axes[1].units = 'mm'\n",
    "    xrf_map.axes_manager.navigation_axes[1].scale = 0.025\n",
    "\n",
    "    xrf_map.axes_manager.signal_axes[0].name = 'Energy'\n",
    "    xrf_map.axes_manager.signal_axes[0].units = 'KeV'\n",
    "    xrf_map.axes_manager.signal_axes[0].scale = 13.717/4096\n",
    "    \n",
    "    return xrf_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Axes manager, axes: (201, 201|4096)>\n",
      "            Name |   size |  index |  offset |   scale |  units \n",
      "================ | ====== | ====== | ======= | ======= | ====== \n",
      "               x |    201 |      0 |       0 |   0.025 |     mm \n",
      "               y |    201 |      0 |       0 |   0.025 |     mm \n",
      "---------------- | ------ | ------ | ------- | ------- | ------ \n",
      "          Energy |   4096 |      0 |       0 |  0.0033 |    KeV \n"
     ]
    }
   ],
   "source": [
    "xrf_map = convert_xrf_to_hyperspy(xrf_data)\n",
    "print(xrf_map.axes_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwrite '/Users/user/Documents/Projects/XRF_machine_learning/data/map.hspy' (y/n)?\n",
      "y\n"
     ]
    }
   ],
   "source": [
    "xrf_map.save(save_path+'map.hspy'.format(map))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 157.47 ms\n"
     ]
    }
   ],
   "source": [
    "xrf_stack = hs.load(save_path+'map.hspy', stack = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Axes manager, axes: (201, 201|4096)>\n",
      "            Name |   size |  index |  offset |   scale |  units \n",
      "================ | ====== | ====== | ======= | ======= | ====== \n",
      "               x |    201 |      0 |       0 |   0.025 |     mm \n",
      "               y |    201 |      0 |       0 |   0.025 |     mm \n",
      "---------------- | ------ | ------ | ------- | ------- | ------ \n",
      "          Energy |   4096 |      0 |       0 |  0.0033 |    KeV \n"
     ]
    }
   ],
   "source": [
    "print(xrf_map.axes_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposition info:\n",
      "  normalize_poissonian_noise=True\n",
      "  algorithm=sklearn_pca\n",
      "  output_dimension=15\n",
      "  centre=None\n",
      "scikit-learn estimator:\n",
      "PCA(n_components=15)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: title={'center': 'data\\nPCA Scree Plot'}, xlabel='Principal component index', ylabel='Proportion of variance'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xrf_stack.decomposition(normalize_poissonian_noise=True, algorithm = 'sklearn_pca', output_dimension=15)\n",
    "xrf_stack.plot_cumulative_explained_variance_ratio()\n",
    "xrf_stack.plot_explained_variance_ratio(log=True, vline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "sig_components = xrf_stack.estimate_elbow_position()\n",
    "print(sig_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# working..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensions of the map\n",
    "also sets save paths..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrf_map.change_dtype('float32')\n",
    "#xrf_map.save(save_path+'map1')\n",
    "\n",
    "xrf_stack = xrf_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Signal1D, title: , dimensions: (201, 201|4096)>"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xrf_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrf_stack.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwrite '/Users/joshuashea/melt_mapsat16_map2_002_mapped_crop.hspy' (y/n)?\n",
      "y\n"
     ]
    }
   ],
   "source": [
    "xrf_stack.change_dtype('float32')\n",
    "\n",
    "\n",
    "xrf_stack.save(save_path+'at16_map2_002_mapped_crop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposition info:\n",
      "  normalize_poissonian_noise=False\n",
      "  algorithm=sklearn_pca\n",
      "  output_dimension=20\n",
      "  centre=None\n",
      "scikit-learn estimator:\n",
      "PCA(n_components=20)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04bbfc07b5f943f3b9530220f44fa788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Label(value='Decomposition component index', layout=Layout(width='15%')), IntSli…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "xrf_stack.decomposition(normalize_poissonian_noise=False, algorithm=\"sklearn_pca\", output_dimension=20)\n",
    "\n",
    "xrf_stack.plot_explained_variance_ratio(log=True, vline=True)\n",
    "\n",
    "xrf_stack.plot_decomposition_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Principal component', ylabel='Cumulative explained variance ratio'>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xrf_stack.plot_cumulative_explained_variance_ratio()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**note** iterative cropping above showed that most data/ analysis was better when cropping down to the shape (157, 63|2001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrf_stack.save(save_path+'lisheen_low_res_map_data_Crop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrf_stack = xrf_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrf_stack.decomposition(normalize_poissonian_noise=True, algorithm='PCA', output_dimension=15)\n",
    "xrf_stack.plot_cumulative_explained_variance_ratio()\n",
    "xrf_stack.plot_explained_variance_ratio(log=True, vline=True)\n",
    "xrf_stack.plot_decomposition_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bic_elbow(signal, decomposition_method='sklearn_pca', n_components_range=np.arange(2, 11)):\n",
    "    \"\"\"\n",
    "    Perform a BIC elbow test to find the optimal number of components for a signal decomposition method.\n",
    "    Parameters\n",
    "    ----------\n",
    "    signal : `hyperspy.signals.Signal2D` or `hyperspy.signals.SignalND`\n",
    "        The hyperspy signal to perform the BIC elbow test on.\n",
    "    decomposition_method : str, optional\n",
    "        The signal decomposition method to use. Default is 'PCA'.\n",
    "    n_components_range : array-like, optional\n",
    "        The range of possible numbers of components to fit the signal model for.\n",
    "        Default is np.arange(2, 11).\n",
    "    Returns\n",
    "    -------\n",
    "    optimal_n_components : int\n",
    "        The optimal number of components that minimizes the BIC score.\n",
    "    Notes\n",
    "    -----\n",
    "    This function uses the Bayesian Information Criterion (BIC) to select the optimal number of components\n",
    "    for the given signal decomposition method. The BIC score is computed for each number of components\n",
    "    in the range, and the elbow point in the BIC curve is identified as the optimal number of components.\n",
    "    \"\"\"\n",
    "    bic_scores = []\n",
    "    for n_components in n_components_range:\n",
    "        # Fit the signal model\n",
    "        if decomposition_method == 'sklearn_pca':\n",
    "            model = signal.decomposition.PCA(n_components=n_components)\n",
    "        elif decomposition_method == 'NMF':\n",
    "            model = signal.decomposition.nmf(n_components=n_components)\n",
    "        elif decomposition_method == 'ICA':\n",
    "            model = signal.decomposition.ica(n_components=n_components)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown decomposition method: {decomposition_method}\")\n",
    "\n",
    "        # Compute the log-likelihood and number of model parameters\n",
    "        log_likelihood = model.score(signal)\n",
    "        n_samples, n_features = signal.data.shape\n",
    "        n_params = n_components * (n_features + 1)\n",
    "\n",
    "        # Compute the BIC score\n",
    "        bic = -2 * log_likelihood + n_params * np.log(n_samples)\n",
    "        bic_scores.append(bic)\n",
    "\n",
    "    # Plot the BIC scores as a function of the number of components\n",
    "    plt.plot(n_components_range, bic_scores, 'o-')\n",
    "    plt.xlabel('Number of components')\n",
    "    plt.ylabel('BIC score')\n",
    "    plt.title(f'BIC elbow test for {decomposition_method} decomposition')\n",
    "    plt.show()\n",
    "\n",
    "    # Find the elbow point\n",
    "    diff = np.diff(bic_scores)\n",
    "    elbow_index = np.argmax(diff) + 1\n",
    "    optimal_n_components = n_components_range[elbow_index]\n",
    "    print(f'The optimal number of components is {optimal_n_components}')\n",
    "\n",
    "    return optimal_n_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'PCA'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbic_elbow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxrf_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecomposition_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msklearn_pca\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_components_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[32], line 27\u001b[0m, in \u001b[0;36mbic_elbow\u001b[0;34m(signal, decomposition_method, n_components_range)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n_components \u001b[38;5;129;01min\u001b[39;00m n_components_range:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# Fit the signal model\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m decomposition_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msklearn_pca\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 27\u001b[0m         model \u001b[38;5;241m=\u001b[39m \u001b[43msignal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecomposition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPCA\u001b[49m(n_components\u001b[38;5;241m=\u001b[39mn_components)\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m decomposition_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNMF\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     29\u001b[0m         model \u001b[38;5;241m=\u001b[39m signal\u001b[38;5;241m.\u001b[39mdecomposition\u001b[38;5;241m.\u001b[39mnmf(n_components\u001b[38;5;241m=\u001b[39mn_components)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'PCA'"
     ]
    }
   ],
   "source": [
    "bic_elbow(xrf_map, decomposition_method='sklearn_pca', n_components_range=np.arange(2, 11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown decomposition method: PCA",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m     xrf_data \u001b[38;5;241m=\u001b[39m h5[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/xrmmap/mcasum/counts\u001b[39m\u001b[38;5;124m'\u001b[39m][:]\n\u001b[1;32m      4\u001b[0m xrf_map \u001b[38;5;241m=\u001b[39m hs\u001b[38;5;241m.\u001b[39msignals\u001b[38;5;241m.\u001b[39mSignal1D(xrf_data)\n\u001b[0;32m----> 6\u001b[0m optimal_n_components \u001b[38;5;241m=\u001b[39m \u001b[43mbic_elbow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxrf_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecomposition_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPCA\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_components_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 33\u001b[0m, in \u001b[0;36mbic_elbow\u001b[0;34m(signal, decomposition_method, n_components_range)\u001b[0m\n\u001b[1;32m     31\u001b[0m     model \u001b[38;5;241m=\u001b[39m signal\u001b[38;5;241m.\u001b[39mdecomposition\u001b[38;5;241m.\u001b[39mica(n_components\u001b[38;5;241m=\u001b[39mn_components)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown decomposition method: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdecomposition_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Compute the log-likelihood and number of model parameters\u001b[39;00m\n\u001b[1;32m     36\u001b[0m log_likelihood \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mscore(signal)\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown decomposition method: PCA"
     ]
    }
   ],
   "source": [
    "with h5py.File(data, 'r') as h5:\n",
    "    xrf_data = h5['/xrmmap/mcasum/counts'][:]\n",
    "\n",
    "xrf_map = hs.signals.Signal1D(xrf_data)\n",
    "\n",
    "optimal_n_components = bic_elbow(xrf_map, decomposition_method='PCA', n_components_range=np.arange(2, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## re-explore\n",
    "\n",
    "get 6 good factors... need to label peaks etc but starting to pull out data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joshuashea/xraylarch/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:289: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposition info:\n",
      "  normalize_poissonian_noise=False\n",
      "  algorithm=NMF\n",
      "  output_dimension=5\n",
      "  centre=None\n",
      "scikit-learn estimator:\n",
      "NMF(n_components=5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joshuashea/xraylarch/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1637: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcdb581e536645a5b27c857af180b408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Label(value='Decomposition component index', layout=Layout(width='15%')), IntSli…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xrf_stack.decomposition(normalize_poissonian_noise=False, algorithm='NMF', output_dimension=5)\n",
    "\n",
    "\n",
    "xrf_stack.plot_decomposition_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrf_stack.decomposition(normalize_poissonian_noise=True, algorithm='NMF', output_dimension=5)\n",
    "\n",
    "\n",
    "xrf_stack.plot_decomposition_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## examine with factor analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposition info:\n",
      "  normalize_poissonian_noise=False\n",
      "  algorithm=Pipeline(steps=[('FA', FactorAnalysis(n_components=5, rotation='varimax'))])\n",
      "  output_dimension=11\n",
      "  centre=None\n",
      "scikit-learn estimator:\n",
      "Pipeline(steps=[('FA', FactorAnalysis(n_components=5, rotation='varimax'))])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4941caacdc514b1db784b6f60801988a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Label(value='Decomposition component index', layout=Layout(width='15%')), IntSli…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline = Pipeline([(\"FA\", FactorAnalysis(n_components=5,rotation = 'varimax'))])\n",
    "\n",
    "xrf_stack.decomposition(normalize_poissonian_noise=False, algorithm=pipeline, return_info=True,output_dimension=11)\n",
    "\n",
    "xrf_stack.plot_decomposition_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d551df69a1e645369ec1cc9ddb3d4477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Label(value='Decomposition component index', layout=Layout(width='15%')), IntSli…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xrf_stack.plot_decomposition_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa_load= xrf_stack.get_decomposition_loadings()\n",
    "#fa_load.save('snv_eds_analysis/plage_TD_eds_FA_4_load')\n",
    "\n",
    "fa_fact= xrf_stack.get_decomposition_factors()\n",
    "#fa_fact.set_elements(elements)\n",
    "\n",
    "\n",
    "#fa_fact.save('snv_eds_analysis/plage_TD_eds_FA_4_fact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa_facts, ydim,xdim=fa_load.data.shape\n",
    "\n",
    "fact_load_vect= pd.DataFrame((fa_load.data.reshape(fa_facts, ydim*xdim).T), columns = ['Factor 1','Factor 2','Factor 3','Factor 4'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fact_load_vect' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [126], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfact_load_vect\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fact_load_vect' is not defined"
     ]
    }
   ],
   "source": [
    "fact_load_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa_facts, ydim,xdim=fa_load.data.shape\n",
    "fa_vect=fa_load.data.reshape(fa_facts,ydim*xdim).transpose()\n",
    "fa_vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings_to_cluster=fa_vect[:,:fa_facts]\n",
    "loadings_to_cluster.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Probabilistic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mPGK\u001b[39;00m(\u001b[43mProbabilistic\u001b[49m, GustafsonKesselMixin):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Probabilistic' is not defined"
     ]
    }
   ],
   "source": [
    "class PGK(Probabilistic, GustafsonKesselMixin):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clus=bic\n",
    "pgk = PGK(n_clusters =num_clus, n_init=10).fit(loadings_to_cluster)\n",
    "# Process results for visualisation\n",
    "print(pgk.memberships_)\n",
    "labels_ = np.argmax(pgk.memberships_, axis=1)\n",
    "memberships_ = pgk.memberships_[range(len(pgk.memberships_)), labels_] \n",
    "\n",
    "labels = labels_.reshape([ydim,xdim])\n",
    "labels=hs.signals.Signal2D(labels)\n",
    "labels.plot(cmap='tab10')\n",
    "\n",
    "mems=pgk.memberships_.reshape(ydim,xdim,num_clus)\n",
    "print(mems.shape)\n",
    "\n",
    "mem_maps=hs.signals.Signal2D(mems)\n",
    "mem_maps=mem_maps.transpose(signal_axes=(2,0))\n",
    "mem_maps.change_dtype('float32')\n",
    "mem_maps.plot(cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "\n",
    "mem_maps.plot(cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## next steps\n",
    "\n",
    "1. makes sense to calcualte an elbow test or examine the BIC to determine the 'optimal' number of clusters\n",
    "2. explore applying HDBSCAN on the FA/ PCA factors\n",
    "3. would also to explore the UMAP pretreatment then HDBSCAN\n",
    "4. maybe blind source separation (ie ICA) after PCA.\n",
    "\n",
    "for 2 and 3 need a different virtual enviroment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
